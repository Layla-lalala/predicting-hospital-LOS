{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOm+x7C6c9rdlA1wP3xpYa+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install ydata-profiling openpyxl"],"metadata":{"id":"vlD1YEj7548T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###########################################################################################\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"e7lUOXkav6vS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q category_encoders scikit-learn pandas numpy joblib"],"metadata":{"id":"Xc5LVlPzWcmv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install optuna"],"metadata":{"id":"5a0Y6TKaXCni"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install catboost optuna -q"],"metadata":{"id":"Cjn-Ca2hkMOv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","import holidays\n","from sklearn.model_selection import train_test_split\n","from sklearn.compose import ColumnTransformer\n","from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n","from sklearn.pipeline import Pipeline\n","from sklearn.impute  import SimpleImputer\n","from joblib import dump\n","import os\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import datetime\n","import re\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import (train_test_split,KFold, StratifiedKFold,cross_val_score, cross_validate)\n","from sklearn.linear_model import LinearRegression\n","import joblib\n","from sklearn.model_selection import TimeSeriesSplit\n","from sklearn.linear_model   import LogisticRegression\n","from sklearn.ensemble     import RandomForestClassifier\n","from sklearn.metrics      import accuracy_score, roc_auc_score\n","from sklearn.preprocessing import FunctionTransformer\n","from sklearn.compose import ColumnTransformer\n","import json, joblib\n","from pathlib import Path\n","\n","from sklearn.linear_model import LinearRegression, Ridge, Lasso\n","from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n","\n","from sklearn.model_selection import KFold\n","from sklearn.ensemble import RandomForestRegressor\n","import optuna\n","from xgboost import XGBRegressor\n","from catboost import CatBoostRegressor, Pool\n","from sklearn.model_selection import RepeatedKFold\n","from sklearn.neural_network import MLPRegressor\n","from optuna.samplers import TPESampler\n","from sklearn.inspection import permutation_importance\n","from sklearn.metrics import make_scorer\n","import shap\n","from sklearn.metrics import median_absolute_error, accuracy_score, confusion_matrix, classification_report, f1_score\n","from sklearn.inspection import PartialDependenceDisplay, partial_dependence\n","\n","import numpy as np, pandas as pd, json, re, joblib\n","import matplotlib.pyplot as plt\n","from pathlib import Path\n","from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n","from sklearn.metrics import brier_score_loss, roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n","\n","from sklearn.feature_selection import VarianceThreshold, SelectPercentile, mutual_info_regression, mutual_info_classif\n","from scipy.stats import probplot\n","from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n"],"metadata":{"id":"ql7smrCIRj3D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/LOSproject/LoS_df_final.xlsx')\n","df.columns = df.columns.str.strip()\n","\n","\"\"\"\n","First\n","By observing, it was found that there are two columns related to the target column.\n","One is the \"LengthOfStay\" column, and the other is the \"SWFT_LoS\" column.\n","Based on speculation, it seems that these are the stay times automatically generated by the hospital system.\n","Therefore, I will manually calculate the exact LengthOfStay value and then decide which column will be the final target column.\n","\"\"\"\n","#Convert the \"Admission/Discharge\" columns to datetime format for subsequent calculations.\n","df[\"AdmissionDateTime\"]  = pd.to_datetime(df[\"AdmissionDateTime\"])\n","df[\"DischargeDateTime\"]  = pd.to_datetime(df[\"DischargeDateTime\"])\n","\n","#Calculate the standard version LOS\n","df[\"LOS_computed\"] = (df[\"DischargeDateTime\"] - df[\"AdmissionDateTime\"]).dt.days\n","\n","#Compare the basic differences of the three types of LOS\n","print(df[[\"LOS_computed\", \"LengthOfStay\", \"SWFT_LoS\"]].describe())\n","\n","#Check the differences in a few actual values\n","print(df[[\"AdmissionDateTime\",\"DischargeDateTime\",\"LOS_computed\",\"LengthOfStay\",\"SWFT_LoS\"]]\n","      .head(10))\n"],"metadata":{"id":"n7YSTI4_fbiQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Classify the columns，Static demographic characteristics\n","I have already checked the differences between the two columns of LOS.\n","The \"LengthOfStay\" column in the table is the same as the calculated time, except for a slight difference in distribution,which is caused by the rounding down method I used.\n","Therefore, from now on, we will directly use the \"LengthOfStay\" column as the target column.\n","\"\"\"\n","\n","#Static demographic characteristics\n","static_cols = [\n","    \"Age (years):\",\n","    \"PersonGender\",\n","    \"PersonBirthDate\",\n","    \"PersonEthnicCategoryDesc\",\n","    \"Postcode\",\n","    \"PrimaryProcedureCode\",\n","    \"PrimaryProcedureDesc\",\n","    \"PatientClassificationDesc\",\n","    \"PRIORITY_TYPE_DESCRIPTION\",\n","    \"Counter_PreviousOrmisActivity\"\n","]\n","\n","#The human measurement group (will be cleaned up later)\n","anthro_cols = [\n","    \"Height (please input in centimeters)\",\n","    \"Weight (please input in kilograms)\",\n","    \"Height CM CORRECTED\",\n","    \"Weight KG CORRECTED\",\n","    \"BMI\"\n","]\n","\n","#Behavioral / PreoperativeQuestionnaireCharacteristics\n","behavior_cols = [\n","    \"Smoker\",\n","    \"Do you live alone?\",\n","    \"Will there be someone, such as a family member or friend, who can support you during your initial recovery?\",\n","    \"Do you have stairs at home?\",\n","    \"Can you go up and down the stairs without assistance?\",\n","    \"Do you have downstairs access to a toilet?\",\n","    \"How far can you currently walk?\",\n","    \"Do you currently use any walking aids to walk?\",\n","    \"Have you had any falls in the past 12 months?\",\n","    \"How long do you expect to stay in hospital after your operation?\",\n","    \"How anxious are you about having joint replacement surgery?\",\n","    \"Have you had previous joint replacement surgery?\",\n","    \"How long do you estimate it would take you to stand from a chair, walk 5 steps away from the chair and then return to the chair again?\"\n","]\n","\n","#Point-in-time information（Admission time, discharge time, start time of surgery, end time of surgery）\n","time_cols = [\n","    \"AdmissionDateTime\",\n","    \"DischargeDateTime\",\n","    \"OperationStartDateTime\",\n","    \"OperationEndDateTime\"\n","]\n","\n","#Surgical-related characteristics（minute）\n","surgical_cols = [\n","    \"OperationLengthMinute\",\n","    \"PatientTotalTime\",\n","]\n","\n","#binary flags (e.g. ['Flag_IsDiagnosedDiabetes', 'Flag_IsDiagnosedHypertension', ...])\n","flag_cols = [c for c in df.columns if c.startswith(\"Flag_IsDiagnosed\")]\n","\n","#Administrative/Location Information Characteristics\n","admin_cols = [\n","    \"ProfCarerOnAdmitName\",     #The name of the main nurse at the time of admission\n","    \"TheatreCode\",          #Operating room number\n","    \"TheatreName\",          #Operating room name\n","    \"AdmitWard\"           #Inpatient ward\n","]\n","\n","#Target column (length of hospital stay)\n","target_cols = [\"LengthOfStay\"]\n","\n","#Have a complete check-up\n","groups = {\n","    \"static\":      static_cols,\n","    \"anthro\":      anthro_cols,\n","    \"behavior\":    behavior_cols,\n","    \"time\":        time_cols,\n","    \"surgical\":    surgical_cols,\n","    \"flags\":       flag_cols,\n","    \"admin\":       admin_cols\n","}\n","\n","for name, cols in groups.items():\n","    missing = set(cols) - set(df.columns)\n","    print(f\"{name:8s} ({len(cols)} cols): missing in df? {missing}\")\n"],"metadata":{"id":"esT8w1qaBD_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Previously, the variable \"time_cols\" has been defined, and all the columns within it have been converted to datetime64 format.\n","#Set errors='coerce', so that the time of parsing failure is changed to NaT, which is convenient for subsequent checks.\n","for c in time_cols:\n","    df[c] = pd.to_datetime(df[c], errors='coerce')\n","\n","#Check for any missing elements or parsing errors.\n","print(df[time_cols].info())\n","\n","#Use the existing \"LengthOfStay\" as the target\n","print(\"LengthOfStay:\", df['LengthOfStay'].describe())\n","\n","#Extract admission / discharge derived features\n","df['Adm_Month']    = df['AdmissionDateTime'].dt.month\n","df['Adm_Weekday']   = df['AdmissionDateTime'].dt.dayofweek\n","df['Adm_Quarter']   = df['AdmissionDateTime'].dt.quarter\n","df['Adm_Hour']    = df['AdmissionDateTime'].dt.hour\n","df['Adm_IsWeekend']  = (df['Adm_Weekday'] >= 5).astype(int)\n","\n","df['Disch_Month']   = df['DischargeDateTime'].dt.month\n","df['Disch_Weekday']  = df['DischargeDateTime'].dt.dayofweek\n","df['Disch_Quarter']  = df['DischargeDateTime'].dt.quarter\n","df['Disch_Hour']   = df['DischargeDateTime'].dt.hour\n","df['Disch_IsWeekend'] = (df['Disch_Weekday'] >= 5).astype(int)\n","\n","#Extract the derived features of the start / end of the surgery\n","df['OpStart_Month']  = df['OperationStartDateTime'].dt.month\n","df['OpStart_Weekday'] = df['OperationStartDateTime'].dt.dayofweek\n","df['OpStart_Hour']  = df['OperationStartDateTime'].dt.hour\n","df['OpStart_IsWeekend']= (df['OpStart_Weekday'] >= 5).astype(int)\n","\n","df['OpEnd_Month']   = df['OperationEndDateTime'].dt.month\n","df['OpEnd_Weekday']  = df['OperationEndDateTime'].dt.dayofweek\n","df['OpEnd_Hour']   = df['OperationEndDateTime'].dt.hour\n","df['OpEnd_IsWeekend'] = (df['OpEnd_Weekday'] >= 5).astype(int)\n","\n","#Mark British statutory holidays\n","uk_hols = holidays.UnitedKingdom()\n","df['Adm_IsHoliday']     = df['AdmissionDateTime'].dt.date.isin(uk_hols).astype(int)\n","df['Disch_IsHoliday']   = df['DischargeDateTime'].dt.date.isin(uk_hols).astype(int)\n","df['OpStart_IsHoliday'] = df['OperationStartDateTime'].dt.date.isin(uk_hols).astype(int)\n","df['OpEnd_IsHoliday']   = df['OperationEndDateTime'].dt.date.isin(uk_hols).astype(int)\n","\n","#Check which new features have been generated\n","derived = [\n","    'Adm_Month','Adm_Weekday','Adm_Quarter','Adm_Hour','Adm_IsWeekend','Adm_IsHoliday',\n","    'Disch_Month','Disch_Weekday','Disch_Quarter','Disch_Hour','Disch_IsWeekend','Disch_IsHoliday',\n","    'OpStart_Month','OpStart_Weekday','OpStart_Hour','OpStart_IsWeekend','OpStart_IsHoliday',\n","    'OpEnd_Month','OpEnd_Weekday','OpEnd_Hour','OpEnd_IsWeekend','OpEnd_IsHoliday'\n","]\n","print(df[derived].head())"],"metadata":{"id":"yiTofnO3mQnC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","There are many missing values in the \"OperationStartDateTime\" and \"OperationEndDateTime\" columns. After deleting these rows, approximately 14% of the samples were lost.\n","If the subsequent model is sensitive to the sample size, or if this 14% has a higher proportion in certain key populations, it may introduce bias.\n","By observing, it was found that the dates of these two columns were almost the same. That is to say, most people completed their surgeries within one day.\n","In the subsequent model, these two columns should have little contribution to the model.\n","Therefore, I decide to only delete the rows where the admission/departure times were completely missing.\n","\"\"\"\n","\n","#Re-read the original data\n","df = pd.read_excel('/content/drive/MyDrive/Colab Notebooks/LOSproject/LoS_df_final.xlsx')\n","df.columns = df.columns.str.strip()\n","\n","#Check the missing rate of each column\n","missing_rate = df.isnull().mean().sort_values(ascending=False)\n","print(\"The missing rate of each column：\\n\", missing_rate.head(20))\n","\n","#Delete the columns with a high missing rate (higher than 40%)\n","to_drop = missing_rate[missing_rate > 0.40].index.tolist()\n","print(f\"Delete the columns with a missing rate of more than 40%（In total, {len(to_drop)} columns）：\\n\", to_drop)\n","df.drop(columns=to_drop, inplace=True)\n","\n","#1.Fill the numeric features with the median because the median is not sensitive to extreme values.(Exclude the target column \"LengthOfStay\")\n","num_cols = df.select_dtypes(include=[np.number]).columns.drop('LengthOfStay')\n","print(f\"Numerical characteristics (will be filled with the median)：{num_cols.tolist()}\")\n","for col in num_cols:\n","    median = df[col].median()\n","    df[col].fillna(median, inplace=True)\n","\n","#Fill in the categorical features with \"Unknown\".\n","cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n","print(f\"Category characteristics (will be filled with 'Unknown')：{cat_cols}\")\n","for col in cat_cols:\n","    df[col].fillna('Unknown', inplace=True)\n","\n","#The missing flag feature (Flag_*) is handled as 'N'.\n","flag_cols = [c for c in df.columns if c.startswith('Flag_IsDiagnosed')]\n","print(f\"Diagnosis Flag Characteristics (will be filled with 'N')：{flag_cols}\")\n","for col in flag_cols:\n","    df[col].fillna('N', inplace=True)\n","\n","#If there is still a missing of time, simply delete it to ensure the accuracy of subsequent calculations.\n","time_cols = ['AdmissionDateTime','DischargeDateTime']\n","still_missing = df[time_cols].isnull().any()\n","if still_missing.any():\n","    print(\"The time column is missing. Delete these rows.：\\n\", df[df[time_cols].isnull().any(axis=1)][time_cols])\n","    df.dropna(subset=['AdmissionDateTime','DischargeDateTime'], inplace=True)\n","\n","#Recheck: Confirm that there are no missing items.\n","final_missing = df.isnull().mean().loc[lambda x: x>0]\n","print(\"Remaining missing after processing：\\n\", final_missing)"],"metadata":{"id":"naApIhXCLnzX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","After processing, there are still some missing values in the \"LengthOfStay\" column, accounting for approximately 0.019632.\n","Through observation, it was found that both \"AdmissionDateTime\" and \"DischargeDateTime\" are also missing.\n","Therefore, it is impossible to use the manually calculated \"LOS_computed\" to complete the data.\n","Therefore, discard approximately 1.96% of the null values and delete these rows.\n","\"\"\"\n","\n","#Count the number of rows where LengthOfStay is missing\n","num_missing = df['LengthOfStay'].isna().sum()\n","print(f\"The number of rows to be deleted：{num_missing}\")\n","\n","#Delete these lines directly\n","df = df.dropna(subset=['LengthOfStay'])\n","\n","#Check again to ensure nothing has been missed.\n","print(\"After deletion, the number of missing 'LengthOfStay' values：\", df['LengthOfStay'].isna().sum())"],"metadata":{"id":"5PJkaf2JNupv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","Categorical Encoding\n","\"\"\"\n","\n","#Read the DataFrame that has already been cleaned\n","df = df.copy()\n","y  = df['LengthOfStay']\n","X  = df.drop(columns=['LengthOfStay'])\n","\n","#Classify feature types\n","#First, identify all the category columns.\n","cat_cols_all = X.select_dtypes(include=['object','category']).columns.tolist()\n","\n","#(1) Manually list the binary columns. Binary column mapping\n","binary_cols = ['Smoker', 'Do you live alone?', 'Do you have stairs at home?', 'Can you go up and down the stairs without assistance?', 'Do you have downstairs access to a toilet?',\n","               'Have you had any falls in the past 12 months?', 'Have you had previous joint replacement surgery?', 'PRIORITY_TYPE_DESCRIPTION','PersonGender',\n","               'Flag_IsDiagnosedDiabetes', 'Flag_IsDiagnosedHypertension', 'Flag_IsDiagnosedSmoker', 'Flag_IsDiagnosedHeartRate', 'Flag_IsDiagnosedMentalandBehaviourAlcohol',\n","               'Flag_IsDiagnosedObesity', 'Flag_IsDiagnosedDisorderOfBrain']\n","binary_map  = {'Yes':1, 'No':0, 'Male':1, 'Female':0, 'Routine':0,'Urgent':1, 'Y':1, 'N':0}\n","\n","for col in binary_cols:\n","    if col in X.columns:\n","        X[col] = X[col].map(binary_map).astype('Int64')\n","\n","#(2) Ordered multi-category column mapping\n","ordinal_maps = {\n","    'Will there be someone, such as a family member or friend, who can support you during your initial recovery?': {\n","        'No': 0,\n","        'Yes, in the daytime only': 1,\n","        'Yes, including overnight': 2\n","    },\n","    'How far can you currently walk?': {\n","        'In the house only': 0,\n","        'Less than a mile': 1,\n","        'More than a mile': 2\n","    },\n","    'How long do you expect to stay in hospital after your operation?': {\n","        'Go home the same day': 0,\n","        '1 night': 1,\n","        '2 nights': 2,\n","        '3 or more nights': 3\n","    },\n","    'How anxious are you about having joint replacement surgery?': {\n","        'Not at all': 0,\n","        'Mildly': 1,\n","        'Moderately': 2,\n","        'Very': 3\n","    },\n","    'How long do you estimate it would take you to stand from a chair, walk 5 steps away from the chair and then return to the chair again?': {\n","        'Unable to do it': 0,\n","        'Less than 15 seconds': 1,\n","        'More than 15 seconds': 2\n","    }\n","}\n","ordinal_cols = []\n","for col, mp in ordinal_maps.items():\n","    if col in X.columns:\n","        X[col] = X[col].map(mp).astype('Int64')\n","        ordinal_cols.append(col)\n","\n","#(3) Other category breakdown\n","# Unprocessed category column\n","cat_cols_all = X.select_dtypes(include=['object','category']).columns.tolist()\n","unprocessed  = [c for c in cat_cols_all if c not in ordinal_cols]\n","\n","# Base number≤20 → One-Hot；   >20 → TargetEncoder\n","ohe_cols   = [c for c in unprocessed if X[c].nunique() <= 20]\n","high_cols  = [c for c in unprocessed if X[c].nunique()  > 20]\n","\n","print(f\"OHE   ({len(ohe_cols)}): {ohe_cols}\")\n","\n","tail = ' ...' if len(high_cols) > 6 else ''\n","print(f\"Target({len(high_cols)}): {high_cols[:6]}{tail}\")\n"],"metadata":{"id":"bhcIVxeoNu_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Preprocessing pipeline\n","#1) Numeric column: (Excluding ordinal_cols as they have already been mapped to Int64)\n","numeric_cols = (\n","    X.select_dtypes(include=[np.number])\n","      .columns.difference(ordinal_cols)\n","      .tolist()\n",")\n","\n","#2) Create a pipeline for the \"ordinal\" column, first calculate the mode → pass through\n","ord_pipe = Pipeline([\n","    ('impute', SimpleImputer(strategy='most_frequent')),\n","    ('id',     'passthrough')\n","])\n","\n","preprocessor = ColumnTransformer(\n","    transformers=[\n","        ('num', SimpleImputer(strategy='median'),       numeric_cols),\n","        ('bin', SimpleImputer(strategy='most_frequent'),    binary_cols),\n","        ('ord', ord_pipe,                   ordinal_cols),\n","        ('ohe', OneHotEncoder(handle_unknown='ignore',\n","                  sparse_output=True),          ohe_cols),\n","        ('te',  TargetEncoder(cols=high_cols,\n","                    smoothing=0.3,\n","                    handle_missing='value'),      high_cols)\n","    ],\n","    remainder='drop',\n","    sparse_threshold=0.3\n",")\n","\n","#3) Run a test and check the shape of the result\n","X_processed = preprocessor.fit_transform(X, y)\n","print('Encoded shape:', X_processed.shape)\n","\n","#4) Save artefacts\n","out_dir = '/content/drive/MyDrive/Colab Notebooks/LOSproject/artifacts'\n","os.makedirs(out_dir, exist_ok=True)\n","\n","#Save the cleaned and mapped dataframe\n","df.to_csv(f'{out_dir}/df_after_encoding.csv', index=False)\n","\n","#Preprocessor snapshot, facilitating subsequent model training\n","dump(preprocessor, f'{out_dir}/los_preprocessor.joblib')\n","print(f'Artifacts saved to: {out_dir}')"],"metadata":{"id":"Rir8uyQANvhn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["####################################################################################################################################################\n","\n","#Second part, exploratory data analysis is conducted, mainly focusing on cleaning the encoded data.\n","#In order to prevent data leakage, after dividing the training set and the test set, only the training set is analyzed.\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/LOSproject/artifacts/df_after_encoding.csv')\n","\n","#Divide the dataset into a training set and a test set (80%/20% split)\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","#View the basic information of the training set\n","print(\"Number of training set samples：\", train_df.shape[0])\n","print(\"Number of test set samples：\", test_df.shape[0])\n","print(\"The fields included in the training set：\", train_df.columns.tolist())\n","#Display the first 5 rows of the training set as examples\n","train_df.head(5)"],"metadata":{"id":"SbFROl-yDEFy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Analyze the distribution characteristics of the target variable \"LengthOfStay\":\n","#Draw its histogram and box plot separately to determine whether there is a long-tailed distribution or extreme values in the data.\n","sns.set_style('whitegrid')\n","\n","#Draw the histogram and box plot of LengthOfStay\n","fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n","\n","#1: Histogram (with kernel density curve)\n","sns.histplot(train_df['LengthOfStay'], kde=True, ax=axes[0], color='skyblue', edgecolor='black')\n","axes[0].set_title(\"LengthOfStay Distribution histogram\")\n","axes[0].set_xlabel(\"LengthOfStay）\")\n","axes[0].set_ylabel(\"frequency\")\n","\n","#2: Box Plot\n","sns.boxplot(y=train_df['LengthOfStay'], ax=axes[1], color='lightgreen')\n","axes[1].set_title(\"LengthOfStay Box Plot\")\n","axes[1].set_ylabel(\"LengthOfStay\")\n","axes[1].set_xlabel(\"\")\n","\n","plt.tight_layout()\n","plt.show()\n","\n","#Save them to Google drive\n","eda_dir = '/content/drive/MyDrive/Colab Notebooks/LOSproject/eda'\n","os.makedirs(eda_dir, exist_ok=True)\n","\n","timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n","out_file  = f'{eda_dir}/LOS_hist_box_{timestamp}.png'\n","fig.savefig(out_file, dpi=300)\n","\n","print(f\"Figure saved to: {out_file}\")"],"metadata":{"id":"7EHuNAqWVK12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Next, I will explore whether the relationship between the target variable \"LengthOfStay\" and other numerical features is linear.\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/LOSproject/artifacts/df_after_encoding.csv')\n","train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n","\n","eda_dir = '/content/drive/MyDrive/Colab Notebooks/LOSproject/eda'\n","os.makedirs(eda_dir, exist_ok=True)\n","\n","#1. The relationship between Age and LengthOfStay\n","age_cols = [c for c in train_df.columns if re.search(r'age', c, re.I)]\n","if age_cols:\n","    age_col = age_cols[0]\n","\n","    tmp_df = train_df[[age_col,'LengthOfStay']].copy()\n","    tmp_df[age_col] = pd.to_numeric(tmp_df[age_col], errors='coerce')\n","    tmp_df = tmp_df.dropna(subset=[age_col,'LengthOfStay'])\n","\n","    plt.figure(figsize=(6,4))\n","    sns.regplot(x=age_col, y='LengthOfStay', data=tmp_df,\n","                lowess=True, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n","    plt.title(f\"Scatter plot of {age_col} and LengthOfStay\")\n","    plt.xlabel(age_col);  plt.ylabel(\"LengthOfStay\")\n","    plt.tight_layout()\n","    # >>>Save\n","    ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n","    path = f'{eda_dir}/Age_LengthOfStay_{ts}.png'\n","    plt.savefig(path, dpi=300);  plt.show()\n","    print(\"Saved:\", path)\n","else:\n","    print(\"No column containing 'age' found, skip Age–LOS plot.\")\n"],"metadata":{"id":"PBKPzcPpgL9g"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#2. The relationship between BMI and LengthOfStay\n","bmi_cols = [c for c in train_df.columns if re.search(r'bmi', c, re.I)]\n","if bmi_cols:\n","    bmi_col = bmi_cols[0]\n","\n","    tmp_df = train_df[[bmi_col,'LengthOfStay']].copy()\n","    tmp_df[bmi_col] = pd.to_numeric(tmp_df[bmi_col], errors='coerce')\n","    tmp_df = tmp_df.dropna(subset=[bmi_col,'LengthOfStay'])\n","\n","    plt.figure(figsize=(6,4))\n","    sns.regplot(x=bmi_col, y='LengthOfStay', data=tmp_df,\n","                lowess=True, scatter_kws={'alpha':0.5}, line_kws={'color':'red'})\n","    plt.title(f\"Scatter plot of {bmi_col} and LengthOfStay\")\n","    plt.xlabel(bmi_col);  plt.ylabel(\"LengthOfStay\")\n","    plt.tight_layout()\n","    #>>> Save\n","    ts  = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n","    path = f'{eda_dir}/BMI_LengthOfStay_{ts}.png'\n","    plt.savefig(path, dpi=300);  plt.show()\n","    print(\"Saved:\", path)\n","else:\n","    print(\"No column containing 'BMI' found, skip BMI–LOS plot.\")"],"metadata":{"id":"DWxnPkgGNBZi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#3. The relationship between categorical variables and LengthOfStay\n","categorical_vars = ['Smoker', 'Support', 'Gender', 'PriorityType']\n","cats_present = [c for c in categorical_vars if c in train_df.columns]\n","\n","if cats_present:\n","    #Put two pictures in each row\n","    n = len(cats_present)\n","    cols = 2\n","    rows = (n + cols - 1) // cols\n","    fig, axes = plt.subplots(rows, cols, figsize=(6*cols, 4*rows))\n","    axes = axes.flatten()\n","\n","    for i, cat in enumerate(cats_present):\n","        sns.boxplot(data=train_df,\n","                    x=cat,\n","                    y='LengthOfStay',\n","                    ax=axes[i],\n","                    palette='Pastel1')\n","        axes[i].set_title(f\"{cat} vs LengthOfStay\")\n","        axes[i].set_xlabel(cat)\n","        axes[i].set_ylabel(\"LengthOfStay\")\n","\n","    for j in range(i+1, len(axes)):\n","        axes[j].axis('off')\n","\n","    plt.tight_layout()\n","\n","    #>>> Save\n","    ts = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n","    path = f'{eda_dir}/Categorical_BoxPlots_{ts}.png'\n","    fig.savefig(path, dpi=300)\n","    plt.show()\n","    print(\"Saved:\", path)\n","else:\n","    print(\"None of the specified categorical variables present.\")"],"metadata":{"id":"WPKxrSmNO9qC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#4. Correlation heatmap of numerical variables\n","\n","#Filter the numerical columns (including the target variable and the encoded numerical features)\n","numeric_cols = train_df.select_dtypes(include=[np.number]).columns\n","corr_matrix  = train_df[numeric_cols].corr(method='pearson')\n","fig, ax = plt.subplots(figsize=(max(8, 0.8*len(numeric_cols)),\n","                                max(6, 0.8*len(numeric_cols))))\n","sns.heatmap(corr_matrix, cmap='YlGnBu', annot=True, fmt=\".2f\",\n","            square=True, ax=ax)\n","ax.set_title(\"Heat map of Pearson correlations\")\n","plt.tight_layout()\n","\n","#>>> Save\n","ts  = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n","path = f'{eda_dir}/NumCorr_Heatmap_{ts}.png'\n","fig.savefig(path, dpi=300);  plt.show()\n","print(\"Saved:\", path)"],"metadata":{"id":"2topCaYXPv_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########################################################################################################################################################\n","\"\"\"\n","Third part\n","Feature Engineering and Experimental Design)\n","\n","3-0  Outlier Handling\n","3-1. Target format selection\n","\"\"\"\n","\n","#Read the file\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/LOSproject/artifacts/df_after_encoding.csv')\n","target_col = 'LengthOfStay'\n","\n","X = df.drop(columns=[target_col])\n","y = df[target_col]\n","X_train, X_test, y_train, y_test = train_test_split(\n","        X, y, test_size=0.2, random_state=42, shuffle=True)\n","\n","#Reassemble the target column back into the training set\n","#which is convenient for conducting feature Exploratory Data Analysis first.\n","train_df = X_train.copy()\n","train_df[target_col] = y_train\n","\n","#--------------    3-0 Outlier Handling   ----------------------\n","class IQRClipper(BaseEstimator, TransformerMixin):\n","    #Perform IQR clipping on the specified column\n","    #new_x = clip(x,  Q1-1.5*IQR,  Q3+1.5*IQR)\n","    def __init__(self, factor=1.5):\n","        self.factor = factor\n","    def fit(self, X, y=None):\n","        #Calculate the upper and lower bounds of the IQR for each numerical column\n","        X = pd.DataFrame(X)\n","        q1  = X.quantile(0.25)\n","        q3  = X.quantile(0.75)\n","        iqr = q3 - q1\n","        self.lower_ = (q1 - self.factor * iqr).values\n","        self.upper_ = (q3 + self.factor * iqr).values\n","        return self\n","    def transform(self, X):\n","        return pd.DataFrame(X).clip(self.lower_, self.upper_, axis=1).values\n","\n","#Specify the column of values to be cropped, LOS will not be cropped.\n","num_for_clip = [c for c in ['Age (years):', 'BMI'] if c in train_df.columns]\n","#First, convert the columns to be cropped into numerical values\n","train_df[num_for_clip] = train_df[num_for_clip].apply(\n","                             pd.to_numeric, errors='coerce')\n","#Perform IQR trimming\n","clipper = IQRClipper()\n","train_df[num_for_clip] = clipper.fit_transform(train_df[num_for_clip])\n","\n","#----------------------  3‑1 Target format derivation   -----------------\n","\n","#Binary classification objective: Short hospital stay vs Long hospital stay (threshold 7 days)\n","#1 indicates long hospital stay (>7 days), while 0 indicates short hospital stay (<=7 days).\n","y_bin = (y_train > 7).astype(int)\n","print(\"Binary classification distribution:\\n\", y_bin.value_counts())\n","\n","y = y_bin\n","\n","\n","#The trained features after trimming\n","X_train_processed = train_df.drop(columns=[target_col])\n","\n","print(\"Outlier clipping done!\")\n","print(\"Shape of X_train:\", X_train_processed.shape)\n","print(\"First few rows:\\n\", X_train_processed.head())\n","\n"],"metadata":{"id":"i2GchbY1DrBH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the running results to Google Drive\n","out_dir = '/content/drive/MyDrive/Colab Notebooks/LOSproject/clean_data'\n","os.makedirs(out_dir, exist_ok=True)\n","\n","#Save the cropped X_train_processed and y_bin\n","X_train_processed.to_parquet(f'{out_dir}/X_train_processed.parquet', index=False)\n","y_bin.to_csv(f'{out_dir}/y_bin.csv', index=False, header=['y_bin'])\n","\n","#Save the fitting parameters of the clipper (IQRClipper) for reuse in online or other experiments.\n","joblib.dump(clipper, f'{out_dir}/iqr_clipper.joblib')\n","\n","print(f'Files saved to: {out_dir}')\n"],"metadata":{"id":"_sKc4fXydyty"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#--------------------------    3‑2 Feature scaling declare numerical / categorical columns   --------------------------------------------\n","\n","ID_COL      = 'ID'\n","EXCLUDE_COL = [\n","    ID_COL, target_col,\n","    'AdmissionDateTime', 'DischargeDateTime',\n","    'LOS_computed', 'SWFT_LoS'\n","]\n","\n","feature_cols = [c for c in train_df.columns if c not in EXCLUDE_COL]\n","\n","# numeric and categorical feature lists\n","numeric_features = [\n","    c for c in ['Age (years):',\n","                'Height CM CORRECTED',\n","                'Weight (please input in kilograms)',\n","                'BMI']\n","    if c in feature_cols\n","]\n","categorical_features = [c for c in feature_cols if c not in numeric_features]\n","\n","print(f\"Numerical features ({len(numeric_features)}):\", numeric_features)\n","print(f\"Categorical features ({len(categorical_features)}):\",\n","      categorical_features[:10], '...' if len(categorical_features) > 10 else '')\n","\n","\n","#--------------------------    3‑3. Cross‑validation schemes   --------------------------------------------\n","cv_reg = KFold(n_splits=5, shuffle=True, random_state=42)\n","cv_cls = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","# (optional) rolling‑window time CV\n","if 'AdmissionDateTime' in train_df.columns:\n","    df_sorted = train_df.sort_values('AdmissionDateTime')\n","    time_cv   = TimeSeriesSplit(n_splits=5)\n","    for i, (tr, val) in enumerate(time_cv.split(df_sorted)):\n","        print(f\"[Time‑CV] Fold {i}: train={len(tr)}, val={len(val)}\")\n","\n","\n","#--------------------------    3‑4. Pre‑processing pipelines  (ColumnTransformer + model)   --------------------------------------------\n","\n","#clean the data\n","clean_numeric = FunctionTransformer(\n","    lambda arr: (\n","        pd.DataFrame(arr)\n","          .applymap(lambda x:\n","              pd.to_numeric(re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce'))\n","          .values),\n","    feature_names_out='one-to-one'\n",")\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   clean_numeric),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","\n","categorical_pipe = Pipeline([\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","\n","])\n","\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features)\n","])\n","\n","\n","#  Baseline models & CV\n","models = {\n","    \"LogReg\": LogisticRegression(max_iter=1000),\n","    \"RF\":     RandomForestClassifier(\n","                  n_estimators=300,\n","                  max_depth=None,\n","                  class_weight=\"balanced\",\n","                  random_state=42)\n","}\n","\n","for name, clf in models.items():\n","    pipe = Pipeline([\n","        ('prep',  preprocessor),\n","        ('model', clf)\n","    ])\n","    acc = cross_val_score(\n","        pipe,\n","        X_train_processed[feature_cols],\n","        y,\n","        cv=cv_cls,\n","        scoring='accuracy'\n","    )\n","    print(f\"{name:<8} | mean ACC: {acc.mean():.3f} ± {acc.std():.3f}\")\n"],"metadata":{"id":"qZCF9-vxDrg2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Saving utilities\n","\n","#Function used in numeric cleaning\n","def clean_numeric_arr(arr):\n","    \"\"\"strip non‑numeric chars and cast to float\"\"\"\n","    return (\n","        pd.DataFrame(arr)\n","          .applymap(lambda x: pd.to_numeric(\n","              re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce'))\n","          .values\n","    )\n","\n","#Re‑create numeric FunctionTransformer (named)\n","clean_numeric = FunctionTransformer(clean_numeric_arr,\n","                                    feature_names_out='one-to-one')\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   clean_numeric),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","\n","preprocessor.set_params(num=numeric_pipe)\n","\n","#Paths\n","save_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments')\n","save_dir.mkdir(parents=True, exist_ok=True)\n","score_path  = save_dir / 'cv_baseline_scores.json'\n","feat_path   = save_dir / 'feature_lists.json'\n","\n","# Save CV scores\n","with open(score_path, 'w') as f:\n","    json.dump(results, f, indent=2)\n","print(f'CV scores saved -> {score_path}')\n","\n","# Save feature name lists\n","with open(feat_path, 'w') as f:\n","    json.dump({'numeric': numeric_features,\n","               'categorical': categorical_features}, f, indent=2)\n","print(f'Feature lists saved -> {feat_path}')\n","\n","#  Fit & save the best pipeline\n","best_name = max(results, key=lambda k: results[k]['mean_acc'])\n","best_clf  = models[best_name]\n","\n","best_pipe = Pipeline([\n","    ('prep',  preprocessor),\n","    ('model', best_clf)\n","])\n"],"metadata":{"id":"F6NBSZjiDsS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########################################################################################################################################################\n","\"\"\"\n","Forth part\n","Model Training and Comparison\n","\n","\n","\"\"\"\n","\n","#-------------------------------------------------    4.1.1 Baseline: Linear Regression    --------------------------------------------------------------------\n","\n","#Load the encoded CSV and define target\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/LOSproject/artifacts/df_after_encoding.csv')\n","target_col = 'LengthOfStay'\n","\n","X = df.drop(columns=[target_col])\n","y = df[target_col]\n","\n","#Split into train / test (80/20)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, shuffle=True\n",")\n","\n","#Re‑assemble train_df for EDA convenience\n","train_df = X_train.copy()\n","train_df[target_col] = y_train\n","\n","#Load numeric and categorical feature lists\n","feat_path = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/feature_lists.json')\n","if feat_path.exists():\n","    with open(feat_path, 'r') as f:\n","        feat = json.load(f)\n","    numeric_features     = feat['numeric']\n","    categorical_features = feat['categorical']\n","else:\n","    ## derive lists automatically\n","    exclude = {'ID', target_col,\n","               'AdmissionDateTime','DischargeDateTime','LOS_computed','SWFT_LoS'}\n","    candidate = [c for c in train_df.columns if c not in exclude]\n","    #numeric = actual numeric dtypes\n","    numeric_features = train_df[candidate].select_dtypes(include=[np.number]).columns.tolist()\n","    categorical_features = [c for c in candidate if c not in numeric_features]\n","\n","#final ordered list of model inputs\n","feature_cols = numeric_features + categorical_features\n","\n","X_reg = train_df[feature_cols].copy()\n","y_reg = train_df[target_col].values\n","\n","print(f\"X_reg shape: {X_reg.shape} | y_reg shape: {y_reg.shape}\")\n","print(f\"Numeric features ({len(numeric_features)}): {numeric_features}\")\n","print(f\"Categorical features ({len(categorical_features)}): {categorical_features[:10]} \"\n","      f\"{'...' if len(categorical_features)>10 else ''}\")\n","\n","#Build preprocessing pipelines（Clean numeric strings such as '83kg' or '1.63 centimetres' to floats，strip non-numeric chars except dot and minus）\n","def clean_numeric_arr(arr):\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(\n","        lambda x: pd.to_numeric(re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce')\n","    ).values\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   FunctionTransformer(clean_numeric_arr, feature_names_out='one-to-one')),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler()),\n","])\n","\n","categorical_pipe = Pipeline([\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","])\n","\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features),\n","])\n","\n","#Combine the preprocessor with any sklearn estimator\n","def make_pipe(estimator):\n","    return Pipeline([\n","        ('prep',  preprocessor),\n","        ('model', estimator)\n","    ])"],"metadata":{"id":"vaH65tNOC6i4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --------------------------------------------------------    4.1.2 Models & alpha grid    ----------------------------------------------------------------------\n","\n","# alpha grid for Ridge / Lasso\n","alphas = [0.01, 0.1, 1.0]\n","\n","# build a dict of estimators\n","models_reg = {\n","    'Linear'      : LinearRegression(),\n","    **{f'Ridge_{a}': Ridge(alpha=a) for a in alphas},\n","    **{f'Lasso_{a}': Lasso(alpha=a, max_iter=5000, tol=1e-4) for a in alphas},\n","}\n","\n","print(\"Prepared regressors:\", list(models_reg.keys()))\n"],"metadata":{"id":"FNoGGY-1EYRW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# --------------------------------------------------------    4.1.3 Repeated 5-fold CV + logging to Drive    ----------------------------------------------------------------------\n","\n","exp_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","exp_dir.mkdir(parents=True, exist_ok=True)\n","\n","#repeat seeds: 3 repeats x 5 folds = 15 runs per model\n","seeds = [42, 2023, 7]\n","\n","# scorers for regression\n","scoring = {\n","    'mae': 'neg_mean_absolute_error',\n","    'mse': 'neg_mean_squared_error',\n","    'r2' : 'r2'\n","}\n","\n","records = []\n","\n","#5-fold CV with shuffling\n","for seed in seeds:\n","    cv = KFold(n_splits=5, shuffle=True, random_state=seed)\n","\n","    for name, est in models_reg.items():\n","        pipe   = make_pipe(est)\n","        cv_res = cross_validate(\n","            pipe,\n","            X_reg, y_reg,\n","            cv=cv,\n","            scoring=scoring,\n","            return_train_score=False,\n","            n_jobs=-1\n","        )\n","\n","        #convert to positive MAE/RMSE per fold\n","        mae_folds  = -cv_res['test_mae']\n","        rmse_folds = np.sqrt(-cv_res['test_mse'])\n","        r2_folds   = cv_res['test_r2']\n","\n","        #log one row per (model, seed)\n","        records.append({\n","            'model'    : name,\n","            'seed'     : seed,\n","            'mae_mean' : float(mae_folds.mean()),\n","            'mae_std'  : float(mae_folds.std()),\n","            'rmse_mean': float(rmse_folds.mean()),\n","            'rmse_std' : float(rmse_folds.std()),\n","            'r2_mean'  : float(r2_folds.mean()),\n","            'r2_std'   : float(r2_folds.std()),\n","            'n_folds'  : int(len(mae_folds))\n","        })\n","\n","        print(f\"{name:10s} | seed {seed} | \"\n","              f\"MAE {mae_folds.mean():.3f}±{mae_folds.std():.3f} | \"\n","              f\"RMSE {rmse_folds.mean():.3f}±{rmse_folds.std():.3f} | \"\n","              f\"R² {r2_folds.mean():.3f}±{r2_folds.std():.3f}\")\n","\n","#save per-seed results\n","results_df = pd.DataFrame(records)\n","results_csv = exp_dir / 'cv_lin_ridge_lasso.csv'\n","results_df.to_csv(results_csv, index=False)\n","print(f\"\\nSaved per-seed CV results -> {results_csv}\")\n","\n","#aggregate across seeds\n","summary = (results_df\n","           .groupby('model', as_index=True)\n","           .agg(mae_mean=('mae_mean', 'mean'),\n","                mae_std =('mae_mean', 'std'),\n","                rmse_mean=('rmse_mean', 'mean'),\n","                rmse_std =('rmse_mean', 'std'),\n","                r2_mean  =('r2_mean', 'mean'),\n","                r2_std   =('r2_mean', 'std'))\n","           .sort_values('rmse_mean'))\n","summary_csv = exp_dir / 'cv_lin_ridge_lasso_summary.csv'\n","summary.round(4).to_csv(summary_csv)\n","print(f\"Saved summary -> {summary_csv}\")\n","\n","#quick view in notebook\n","print(\"\\nTop models by RMSE (lower is better):\")\n","print(summary.round(3).head(10))\n"],"metadata":{"id":"rQvRwnrlGYQQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -------------------------------------    4.1.4 Fit best model on train, evaluate on test, and save artifacts    -------------------------------------------------------\n","\n","\n","exp_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","exp_dir.mkdir(parents=True, exist_ok=True)\n","\n","#pick the best model name by lowest RMSE\n","summary_csv = exp_dir / 'cv_lin_ridge_lasso_summary.csv'\n","summary     = pd.read_csv(summary_csv, index_col=0)\n","if 'summary' not in globals():\n","    summary = pd.read_csv(summary_csv, index_col=0)\n","\n","#pick the first row after sorting by rmse_mean\n","best_name = summary.index[0]\n","print(f\"Best baseline by RMSE: {best_name}\")\n","\n","#Build estimator from its name (fallback in case models_reg is unavailable)\n","def make_estimator_by_name(name: str):\n","    if name == 'Linear':\n","        return LinearRegression()\n","    m = re.match(r'^Ridge_(.*)$', name)\n","    if m:\n","        return Ridge(alpha=float(m.group(1)))\n","    m = re.match(r'^Lasso_(.*)$', name)\n","    if m:\n","        return Lasso(alpha=float(m.group(1)), max_iter=5000, tol=1e-4)\n","    raise ValueError(f\"Unknown model name: {name}\")\n","\n","#Build & fit the pipeline\n","best_est = make_estimator_by_name(best_name)\n","\n","#Fit best pipeline on the full training set used in CV\n","best_pipe = make_pipe(best_est)\n","best_pipe.fit(X_reg[feature_cols], y_reg)\n","\n","#Evaluate on the hold‑out test set\n","X_test_reg = X_test[feature_cols].copy()\n","y_pred = best_pipe.predict(X_test_reg)\n","\n","mae   = mean_absolute_error(y_test, y_pred)\n","mse   = mean_squared_error(y_test, y_pred)\n","rmse  = np.sqrt(mse)\n","r2    = r2_score(y_test, y_pred)\n","\n","metrics = {\n","    'model': best_name,\n","    'mae':   float(mae),\n","    'rmse':  float(rmse),\n","    'r2':    float(r2)\n","}\n","print(f\"[Test] {best_name} | MAE={mae:.3f} | RMSE={rmse:.3f} | R²={r2:.3f}\")\n","\n","\n","#Save artifacts to Drive\n","\n","# (1) fitted pipeline\n","model_path = exp_dir / f'best_baseline_{best_name}.joblib'\n","joblib.dump(best_pipe, model_path, compress=3)\n","\n","# (2) test metrics\n","metrics_path = exp_dir / 'test_metrics_baseline.json'\n","with open(metrics_path, 'w') as f:\n","    json.dump(metrics, f, indent=2)\n","\n","# (3) test predictions\n","pred_df = pd.DataFrame({\n","    'ID'        : X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_test)),\n","    'y_true'    : y_test.values,\n","    'y_pred'    : y_pred,\n","    'abs_error' : np.abs(y_test.values - y_pred),\n","})\n","pred_csv = exp_dir / 'test_predictions_baseline.csv'\n","pred_df.to_csv(pred_csv, index=False)\n","\n","print(f\"Saved pipeline -> {model_path}\")\n","print(f\"Saved test metrics -> {metrics_path}\")\n","print(f\"Saved test predictions -> {pred_csv}\")\n"],"metadata":{"id":"ap78BdIyHpl8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########################################################################################################################################################\n","\n","\n","#-------------------------------------------------    4.2 Random Forest    --------------------------------------------------------------------\n","\n","#-------------------------------------------------    4.2.1 Data preparation and feature loading    --------------------------------------------------------------------\n","\n","#Load the encoded CSV and define target\n","df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/LOSproject/artifacts/df_after_encoding.csv')\n","target_col = 'LengthOfStay'\n","\n","X = df.drop(columns=[target_col])\n","y = df[target_col]\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)\n","\n","#Load feature‐lists.json\n","feat_path = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/feature_lists.json')\n","with open(feat_path, 'r') as f:\n","    feat = json.load(f)\n","\n","numeric_features     = feat['numeric']\n","categorical_features = feat['categorical']\n","\n","#Build X_reg ， y_reg\n","feature_cols = numeric_features + categorical_features\n","X_reg = X_train[feature_cols].copy()\n","y_reg = y_train.values\n","\n","print(\"Loaded:\")\n","print(\"  • X_reg:\", X_reg.shape,\n","      \"  • y_reg:\", y_reg.shape)\n","print(\"  • X_test:\", X_test.shape,\n","      \"  • y_test:\", y_test.shape)\n","print(\"  • numeric features:\", len(numeric_features))\n","print(\"  • categorical features:\", len(categorical_features))"],"metadata":{"id":"LlE_cWXE2vjc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#Save the data for the subsequent use of the model.\n","save_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","save_dir.mkdir(parents=True, exist_ok=True)\n","\n","#Dump the train/test arrays， save X_reg, y_reg, X_test, y_test as .joblib for fast reload\n","joblib.dump(X_reg, save_dir / 'X_reg.joblib')\n","joblib.dump(y_reg, save_dir / 'y_reg.joblib')\n","joblib.dump(X_test, save_dir / 'X_test.joblib')\n","joblib.dump(y_test, save_dir / 'y_test.joblib')\n","\n","#Dump the feature‐lists\n","feat = {\n","    'numeric':     numeric_features,\n","    'categorical': categorical_features\n","}\n","with open(save_dir / 'feature_lists.json', 'w') as f:\n","    json.dump(feat, f, indent=2)\n","\n","print(\"Saved:\")\n","print(\" ·\", save_dir/'X_reg.joblib')\n","print(\" ·\", save_dir/'y_reg.joblib')\n","print(\" ·\", save_dir/'X_test.joblib')\n","print(\" ·\", save_dir/'y_test.joblib')\n","print(\" ·\", save_dir/'feature_lists.json')"],"metadata":{"id":"psWrS1Ir2vxh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------    4.2.2   K-fold cross-validation training    --------------------------------------------------------------------\n","\n","\"\"\"\n","I will employ repeated K-fold cross-validation to obtain a robust performance evaluation.\n","Additionally, I will assess the model's performance on an independent test set and save the predicted results and evaluation metrics from the test set.\n","\"\"\"\n","\n","#List of specified numerical / categorical features\n","numeric_features = [\n","    'Age (years):',\n","    'Height CM CORRECTED',\n","    'Weight (please input in kilograms)',\n","    'BMI',\n","]\n","categorical_features = [\n","    c for c in X_reg.columns if c not in numeric_features\n","]\n","\n","#Rebuild the preprocessor ColumnTransformer\n","def clean_numeric_arr(arr):\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(\n","        lambda x: pd.to_numeric(\n","            re.sub(r'[^0-9.\\-]+', '', str(x)),\n","            errors='coerce')\n","    ).values\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   FunctionTransformer(clean_numeric_arr, feature_names_out='one-to-one')),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","categorical_pipe = Pipeline([\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","])\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features)\n","])\n","\n","#Run the fit_transform operation on X_reg, get the pure numpy array X_proc, shape = (n_samples, n_transformed_features)\n","X_proc = preprocessor.fit_transform(X_reg)\n","y_np   = y_reg.copy()\n","\n","#Define the cross-validation parameters, use 5-fold cross-validation and collect the results of each fold.\n","seeds = [42, 2023, 7]\n","n_splits = 5\n","cv_results = []\n","\n","\n","for seed in seeds:\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","    #Divide the training set and the validation set\n","    for fold, (train_idx, val_idx) in enumerate(kf.split(X_proc), start=1):\n","        X_tr, X_va = X_proc[train_idx], X_proc[val_idx]\n","        y_tr, y_va = y_np[train_idx],   y_np[val_idx]\n","\n","        #Initialize and train the random forest\n","        rf = RandomForestRegressor(\n","            n_estimators=300,\n","            random_state=seed,\n","            n_jobs=-1\n","        )\n","        rf.fit(X_tr, y_tr)\n","        y_pred = rf.predict(X_va)\n","\n","        ##Calculate evaluation indicators and record\n","        mae  = mean_absolute_error(y_va, y_pred)\n","        mse  = mean_squared_error(y_va, y_pred)\n","        rmse = np.sqrt(mse)\n","        r2   = r2_score(y_va, y_pred)\n","        cv_results.append({\n","            'seed' : seed, 'fold': fold,\n","            'MAE'  : mae,\n","            'RMSE' : rmse,\n","            'R2'   : r2\n","        })\n","\n","\n","#Finally, summarize it into a DataFrame.\n","cv_df = pd.DataFrame(cv_results)\n","print(\"CV summary by seed:\")\n","print(cv_df.groupby('seed')[['MAE','RMSE','R2']].agg(['mean','std']))\n"],"metadata":{"id":"bthpRfuV2v_d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Create a new dictionary for random forest model\n","rf_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_baseline\")\n","rf_dir.mkdir(parents=True, exist_ok=True)\n","\n","#Save the detailed results of cross-validation to CSV\n","cv_df.to_csv(rf_dir / \"cv_rf.csv\", index=False)\n","\n","#Calculate the overall mean and standard deviation of each indicator\n","summary_df = cv_df[[\"MAE\", \"RMSE\", \"R2\"]].agg(['mean', 'std']).T\n","\n","#Rename the column names to \"Mean\" and \"Std\"\n","summary_df.columns = [\"Mean\", \"Std\"]\n","summary_df.index.name = \"Metric\"\n","\n","#Save the summary table to CSV\n","summary_df.to_csv(rf_dir / \"cv_rf_summary.csv\")"],"metadata":{"id":"VXudgMhxPq2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------    4.2.3   Test set prediction and model saving    --------------------------------------------------------------------\n","\n","#Rebuild and fit the final Random Forest model on the entire training set\n","final_rf = RandomForestRegressor(\n","    n_estimators=300,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","\n","# Fit the model (with preprocessing)\n","rf_pipe = Pipeline([\n","    ('prep', preprocessor),\n","    ('model', final_rf)\n","])\n","rf_pipe.fit(X_reg, y_reg)\n","\n","#Evaluate on the hold-out test set\n","X_test_reg = X_test[feature_cols].copy()\n","y_pred = rf_pipe.predict(X_test_reg)\n","\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2 = r2_score(y_test, y_pred)\n","\n","print(f\"[Test] Random Forest | MAE={mae:.3f} | RMSE={rmse:.3f} | R²={r2:.3f}\")\n"],"metadata":{"id":"WxdXbFPmRDKs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save artifacts to Drive\n","rf_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_baseline\")\n","rf_dir.mkdir(parents=True, exist_ok=True)\n","\n","#Save the model\n","model_path = rf_dir / 'best_random_forest.joblib'\n","joblib.dump(rf_pipe, model_path, compress=3)\n","print(f\"Saved pipeline -> {model_path}\")\n","\n","#Save the test set metrics\n","metrics = {\n","    'model': 'RandomForest',\n","    'mae': float(mae),\n","    'rmse': float(rmse),\n","    'r2': float(r2)\n","}\n","metrics_path = rf_dir / 'test_metrics_rf.json'\n","with open(metrics_path, 'w') as f:\n","    json.dump(metrics, f, indent=2)\n","print(f\"Saved test metrics -> {metrics_path}\")\n","\n","#Save the prediction results\n","pred_df = pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_test)),\n","    'y_true': y_test.values,\n","    'y_pred': y_pred,\n","    'abs_error': np.abs(y_test.values - y_pred)\n","})\n","pred_csv = rf_dir / 'test_predictions_rf.csv'\n","pred_df.to_csv(pred_csv, index=False)\n","print(f\"Saved test predictions -> {pred_csv}\")"],"metadata":{"id":"uAmZmz-KSCoP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#The model has certain predictive capabilities, but there is still a lot of room for optimization.\n","#Therefore, try again with automatic parameter adjustment.\n","\n","def objective(trial):\n","    #Define the parameter search space\n","    n_estimators = trial.suggest_int('n_estimators', 200, 1000)\n","    max_depth = trial.suggest_int('max_depth', 3, 20)\n","    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n","    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)\n","    max_features = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n","\n","    #Define the model\n","    rf = RandomForestRegressor(\n","        n_estimators=n_estimators,\n","        max_depth=max_depth,\n","        min_samples_split=min_samples_split,\n","        min_samples_leaf=min_samples_leaf,\n","        max_features=max_features,\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","\n","    #Cross-validation (using the negative mean square error as the metric, the lower the RMSE, the better.)\n","    scores = cross_val_score(\n","        Pipeline([('prep', preprocessor), ('model', rf)]),\n","        X_reg, y_reg, cv=5, scoring='neg_root_mean_squared_error', n_jobs=-1\n","    )\n","    return -scores.mean()\n","\n","#Start the Optuna hyperparameter tuning\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=50)\n","\n","print(\"Best trial:\", study.best_trial.params)\n"],"metadata":{"id":"JCUCJTi7SC7r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Automatically input the optimal parameters into the model and re-train it using the entire training set\n","\n","#Take best params from Optuna and build final RF\n","best_params = study.best_params.copy()\n","best_params.update({'random_state': 42, 'n_jobs': -1})\n","rf_best = RandomForestRegressor(**best_params)\n","\n","#Build pipeline (re-use your preprocessor) and fit on full training set\n","rf_pipe_best = Pipeline([\n","    ('prep',  preprocessor),\n","    ('model', rf_best)\n","])\n","rf_pipe_best.fit(X_reg, y_reg)\n","\n","#Evaluate on the hold-out test set\n","X_test_reg = X_test[feature_cols].copy()\n","y_pred = rf_pipe_best.predict(X_test_reg)\n","\n","mae  = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2   = r2_score(y_test, y_pred)\n","\n","print(f\"[Test] RF(Optuna) | MAE={mae:.3f} | RMSE={rmse:.3f} | R²={r2:.3f}\")\n","print(\"Best params:\", best_params)\n","\n"],"metadata":{"id":"R3oqwBRkSDKr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save artifacts to Drive\n","out_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_optuna')\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","#fitted pipeline\n","model_path = out_dir / 'rf_optuna_pipeline.joblib'\n","joblib.dump(rf_pipe_best, model_path, compress=3)\n","\n","#test metrics (+ best params for traceability)\n","metrics = {\n","    'mae': float(mae), 'rmse': float(rmse), 'r2': float(r2),\n","    'best_params': best_params\n","}\n","with open(out_dir / 'rf_optuna_test_metrics.json', 'w') as f:\n","    json.dump(metrics, f, indent=2)\n","\n","#test predictions\n","pred_df = pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_test)),\n","    'y_true': y_test.values,\n","    'y_pred': y_pred,\n","    'abs_error': np.abs(y_test.values - y_pred)\n","})\n","pred_df.to_csv(out_dir / 'rf_optuna_test_predictions.csv', index=False)\n","\n","#full Optuna trial history\n","try:\n","    trials_df = study.trials_dataframe(attrs=('number', 'value', 'params', 'state'))\n","    trials_df.to_csv(out_dir / 'optuna_trials_rf.csv', index=False)\n","except Exception:\n","    pass  # older optuna may not have attrs argument\n","\n","#feature importance\n","try:\n","    feat_names = rf_pipe_best.named_steps['prep'].get_feature_names_out()\n","    importances = rf_pipe_best.named_steps['model'].feature_importances_\n","    fi = (pd.DataFrame({'feature': feat_names, 'importance': importances})\n","          .sort_values('importance', ascending=False))\n","    fi.to_csv(out_dir / 'rf_optuna_feature_importance.csv', index=False)\n","except Exception as e:\n","    print(\"Skip feature importance:\", e)\n","\n","print(f\"Saved pipeline -> {model_path}\")\n","print(f\"Saved metrics  -> {out_dir/'rf_optuna_test_metrics.json'}\")\n","print(f\"Saved preds    -> {out_dir/'rf_optuna_test_predictions.csv'}\")"],"metadata":{"id":"JIPzyvSXSDZX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###############################################################################################################################################################\n","\n","\n","#-------------------------------------------------    4.3 XGBoost    --------------------------------------------------------------------\n","\n","#-------------------------------------------------    4.3.1 Data preparation and feature loading    --------------------------------------------------------------------\n","# Load the prepared train/test data and feature lists from the previous section\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","X_reg = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg = joblib.load(data_dir / 'y_reg.joblib')\n","X_test = joblib.load(data_dir / 'X_test.joblib')\n","y_test = joblib.load(data_dir / 'y_test.joblib')\n","\n","with open(data_dir / 'feature_lists.json', 'r') as f:\n","    feat = json.load(f)\n","numeric_features = feat['numeric']\n","categorical_features = feat['categorical']\n","feature_cols = numeric_features + categorical_features\n","\n","print(\"Loaded saved dataset:\")\n","print(\"  • X_reg:\", X_reg.shape, \"  • y_reg:\", y_reg.shape)\n","print(\"  • X_test:\", X_test.shape, \"  • y_test:\", y_test.shape)\n","print(\"  • numeric features:\", len(numeric_features))\n","print(\"  • categorical features:\", len(categorical_features))"],"metadata":{"id":"Mif5ESN1SDnh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------    4.3.2 K-fold cross-validation training    --------------------------------------------------------------------\n","\"\"\"\n","Similar to the Random Forest, I will perform repeated K-fold cross-validation for XGBoost\n","to evaluate its performance robustly. We will then test on the independent test set.\n","\"\"\"\n","\n","#Define which features are numeric vs categorical\n","numeric_features = [\n","    'Age (years):',\n","    'Height CM CORRECTED',\n","    'Weight (please input in kilograms)',\n","    'BMI',\n","]\n","categorical_features = [c for c in X_reg.columns if c not in numeric_features]\n","\n","#Rebuild the preprocessing pipeline\n","def clean_numeric_arr(arr):\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(\n","        lambda x: pd.to_numeric(re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce')\n","    ).values\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   FunctionTransformer(clean_numeric_arr, feature_names_out='one-to-one')),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","categorical_pipe = Pipeline([\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","])\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features)\n","])\n","\n","\n","#Transform the training features to a numeric array for cross-validation\n","X_proc = preprocessor.fit_transform(X_reg)\n","y_np   = y_reg.copy()\n","\n","#Set up cross-validation parameters\n","seeds = [42, 2023, 7]\n","n_splits = 5\n","cv_results = []\n","\n","# Perform repeated K-fold cross-validation\n","for seed in seeds:\n","    kf = KFold(n_splits=n_splits, shuffle=True, random_state=seed)\n","    for fold, (train_idx, val_idx) in enumerate(kf.split(X_proc), start=1):\n","        X_tr, X_va = X_proc[train_idx], X_proc[val_idx]\n","        y_tr, y_va = y_np[train_idx], y_np[val_idx]\n","\n","        # Initialize and train the XGBoost regressor\n","        xgb_model = XGBRegressor(\n","            n_estimators=300,\n","            random_state=seed,\n","            n_jobs=-1\n","        )\n","        xgb_model.fit(X_tr, y_tr)\n","        y_pred = xgb_model.predict(X_va)\n","\n","        # Calculate evaluation metrics for this fold and record them\n","        mae  = mean_absolute_error(y_va, y_pred)\n","        mse  = mean_squared_error(y_va, y_pred)\n","        rmse = np.sqrt(mse)\n","        r2   = r2_score(y_va, y_pred)\n","        cv_results.append({\n","            'seed': seed, 'fold': fold,\n","            'MAE': mae,\n","            'RMSE': rmse,\n","            'R2': r2\n","        })\n","\n","# Summarize cross-validation results\n","cv_df = pd.DataFrame(cv_results)\n","print(\"CV summary by seed:\")\n","print(cv_df.groupby('seed')[['MAE','RMSE','R2']].agg(['mean','std']))\n"],"metadata":{"id":"ubnw5WPoSD2h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------    4.3.3 Test set prediction and model saving    --------------------------------------------------------------------\n","\n","# Create directory for XGBoost baseline results\n","xgb_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/xgboost_baseline\")\n","xgb_dir.mkdir(parents=True, exist_ok=True)\n","\n","# Save the detailed cross-validation results and summary\n","cv_df.to_csv(xgb_dir / \"cv_xgb.csv\", index=False)\n","summary_df = cv_df[[\"MAE\", \"RMSE\", \"R2\"]].agg(['mean', 'std']).T\n","summary_df.columns = [\"Mean\", \"Std\"]\n","summary_df.index.name = \"Metric\"\n","summary_df.to_csv(xgb_dir / \"cv_xgb_summary.csv\")\n","\n","# Train a final XGBoost model on the entire training set\n","final_xgb = XGBRegressor(\n","    n_estimators=300,\n","    random_state=42,\n","    n_jobs=-1\n",")\n","xgb_pipe = Pipeline([\n","    ('prep', preprocessor),\n","    ('model', final_xgb)\n","])\n","xgb_pipe.fit(X_reg, y_reg)\n","\n","# Evaluate the model on the hold-out test set\n","X_test_reg = X_test[feature_cols].copy()\n","y_pred = xgb_pipe.predict(X_test_reg)\n","mae  = mean_absolute_error(y_test, y_pred)\n","mse  = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2   = r2_score(y_test, y_pred)\n","print(f\"[Test] XGBoost | MAE={mae:.3f} | RMSE={rmse:.3f} | R²={r2:.3f}\")"],"metadata":{"id":"C2L8jFU3IlmV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save the baseline XGBoost model and test results\n","model_path = xgb_dir / 'best_xgboost.joblib'\n","joblib.dump(xgb_pipe, model_path, compress=3)\n","print(f\"Saved XGBoost pipeline -> {model_path}\")\n","\n","# Save test set metrics to JSON\n","metrics = {\n","    'model': 'XGBoost',\n","    'mae': float(mae),\n","    'rmse': float(rmse),\n","    'r2': float(r2)\n","}\n","metrics_path = xgb_dir / 'test_metrics_xgb.json'\n","with open(metrics_path, 'w') as f:\n","    json.dump(metrics, f, indent=2)\n","print(f\"Saved test metrics -> {metrics_path}\")\n","\n","# Save the test set predictions to CSV\n","pred_df = pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_test)),\n","    'y_true': y_test.values,\n","    'y_pred': y_pred,\n","    'abs_error': np.abs(y_test.values - y_pred)\n","})\n","pred_csv = xgb_dir / 'test_predictions_xgb.csv'\n","pred_df.to_csv(pred_csv, index=False)\n","print(f\"Saved test predictions -> {pred_csv}\")"],"metadata":{"id":"q3bn4KwmJmST"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Now, let's perform hyperparameter tuning using Optuna to further improve performance.\n","\n","def objective(trial):\n","    #Define the hyperparameter search space for XGBoost\n","    n_estimators = trial.suggest_int('n_estimators', 100, 500)\n","    max_depth = trial.suggest_int('max_depth', 3, 15)\n","    learning_rate = trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n","    subsample = trial.suggest_float('subsample', 0.5, 1.0, step=0.1)\n","    colsample_bytree = trial.suggest_float('colsample_bytree', 0.5, 1.0, step=0.1)\n","    min_child_weight = trial.suggest_int('min_child_weight', 1, 10)\n","    gamma = trial.suggest_float('gamma', 0.0, 5.0)\n","\n","    #Define the model with given hyperparameters\n","    xgb_model = XGBRegressor(\n","        n_estimators=n_estimators,\n","        max_depth=max_depth,\n","        learning_rate=learning_rate,\n","        subsample=subsample,\n","        colsample_bytree=colsample_bytree,\n","        min_child_weight=min_child_weight,\n","        gamma=gamma,\n","        random_state=42,\n","        n_jobs=-1\n","    )\n","    #Perform 5-fold cross-validation and use negative RMSE as the metric to minimize\n","    scores = cross_val_score(\n","        Pipeline([('prep', preprocessor), ('model', xgb_model)]),\n","        X_reg, y_reg, cv=5,\n","        scoring='neg_root_mean_squared_error',\n","        n_jobs=-1\n","    )\n","    return -scores.mean()\n","\n","#Start the Optuna hyperparameter tuning\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=50)\n","\n","print(\"Best trial hyperparameters:\", study.best_trial.params)\n","\n","#Take the best hyperparameters and train a final XGBoost model on the full training set\n","best_params = study.best_params.copy()\n","best_params.update({'random_state': 42, 'n_jobs': -1})\n","xgb_best = XGBRegressor(**best_params)\n","\n","xgb_pipe_best = Pipeline([\n","    ('prep', preprocessor),\n","    ('model', xgb_best)\n","])\n","xgb_pipe_best.fit(X_reg, y_reg)\n","\n","#Evaluate the tuned model on the test set\n","y_pred = xgb_pipe_best.predict(X_test_reg)\n","mae  = mean_absolute_error(y_test, y_pred)\n","mse  = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2   = r2_score(y_test, y_pred)\n","print(f\"[Test] XGB(Optuna) | MAE={mae:.3f} | RMSE={rmse:.3f} | R²={r2:.3f}\")\n","print(\"Best hyperparameters:\", best_params)\n"],"metadata":{"id":"meLgdl0VJmjH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save artifacts for the tuned XGBoost model\n","optuna_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/xgboost_optuna')\n","optuna_dir.mkdir(parents=True, exist_ok=True)\n","\n","#Save the tuned model pipeline\n","tuned_model_path = optuna_dir / 'xgb_optuna_pipeline.joblib'\n","joblib.dump(xgb_pipe_best, tuned_model_path, compress=3)\n","\n","#Save test metrics (including best hyperparameters) to JSON\n","metrics = {\n","    'mae': float(mae),\n","    'rmse': float(rmse),\n","    'r2': float(r2),\n","    'best_params': best_params\n","}\n","with open(optuna_dir / 'xgb_optuna_test_metrics.json', 'w') as f:\n","    json.dump(metrics, f, indent=2)\n","\n","#Save test set predictions for the tuned model\n","pred_df = pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_test)),\n","    'y_true': y_test.values,\n","    'y_pred': y_pred,\n","    'abs_error': np.abs(y_test.values - y_pred)\n","})\n","pred_df.to_csv(optuna_dir / 'xgb_optuna_test_predictions.csv', index=False)\n","\n","#Save the full Optuna trials history to CSV (if supported)\n","try:\n","    trials_df = study.trials_dataframe(attrs=('number', 'value', 'params', 'state'))\n","    trials_df.to_csv(optuna_dir / 'optuna_trials_xgb.csv', index=False)\n","except Exception:\n","    pass  # Older Optuna versions may not support the attrs parameter\n","\n","#Save feature importance from the tuned model\n","try:\n","    feat_names = xgb_pipe_best.named_steps['prep'].get_feature_names_out()\n","    importances = xgb_pipe_best.named_steps['model'].feature_importances_\n","    fi_df = pd.DataFrame({'feature': feat_names, 'importance': importances})\n","    fi_df.sort_values('importance', ascending=False, inplace=True)\n","    fi_df.to_csv(optuna_dir / 'xgb_optuna_feature_importance.csv', index=False)\n","except Exception as e:\n","    print(\"Skip feature importance:\", e)\n","\n","print(f\"Saved tuned model pipeline -> {tuned_model_path}\")\n","print(f\"Saved tuned model metrics -> {optuna_dir/'xgb_optuna_test_metrics.json'}\")\n","print(f\"Saved tuned model predictions -> {optuna_dir/'xgb_optuna_test_predictions.csv'}\")\n"],"metadata":{"id":"G3kF4oBHJmw9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###############################################################################################################################################################\n","\n","\n","#-------------------------------------------------    4.4 CatBoost    --------------------------------------------------------------------\n","\n","#-------------------------------------------------    4.4.1 Data preparation and feature loading    --------------------------------------------------------------------\n","# Load the prepared train/test data and feature lists from the previous section\n","\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","\n","##Load the training set and test set data.\n","X_reg  = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg  = joblib.load(data_dir / 'y_reg.joblib')\n","X_test = joblib.load(data_dir / 'X_test.joblib')\n","y_test = joblib.load(data_dir / 'y_test.joblib')\n","\n","#Load the list of feature names\n","with open(data_dir / 'feature_lists.json', 'r') as f:\n","    feat = json.load(f)\n","numeric_features     = feat['numeric']\n","categorical_features = feat['categorical']\n","feature_cols = numeric_features + categorical_features\n","\n","print(\"Loaded saved dataset:\")\n","print(\"  • X_reg:\", X_reg.shape, \"  • y_reg:\", y_reg.shape)\n","print(\"  • X_test:\", X_test.shape, \"  • y_test:\", y_test.shape)\n","print(\"  • numeric features:\", len(numeric_features))\n","print(\"  • categorical features:\", len(categorical_features))"],"metadata":{"id":"JM4L8DEXftj8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------    4.4.2 Build a Pipeline for preprocessing and modeling    --------------------------------------------------------------------\n","\n","\"\"\"\n","Since CatBoost can directly handle categorical features, we will fill in the missing values and standardize the numerical features during the preprocessing step,\n","and only fill in the missing values for the categorical features.\n","Then create a custom DataFramePreprocessor converter to ensure that the preprocessed data is returned as a pandas DataFrame,\n","so that CatBoost can correctly identify the feature types:\n","\"\"\"\n","\n","#Transformer for filling and scaling numerical features, filling categorical features, without altering the structure of the DataFrame\n","class DataFramePreprocessor(BaseEstimator, TransformerMixin):\n","    def __init__(self, numeric_features, categorical_features):\n","        self.numeric_features = numeric_features\n","        self.categorical_features = categorical_features\n","        #Numeric feature filler and scaler\n","        self.num_imputer = SimpleImputer(strategy=\"median\")\n","        self.num_scaler = StandardScaler()\n","        #Categorical feature filler (filling missing values with the special string \"__missing__\")\n","        self.cat_imputer = SimpleImputer(strategy=\"constant\", fill_value=\"__missing__\")\n","    def fit(self, X, y=None):\n","        #Fit the filler and scaler only on the training set\n","        self.num_imputer.fit(X[self.numeric_features])\n","        #Fill in the numerical features, then perform fitting and scaling.\n","        X_num_imputed = self.num_imputer.transform(X[self.numeric_features])\n","        self.num_scaler.fit(X_num_imputed)\n","        #Category feature filler fitting\n","        self.cat_imputer.fit(X[self.categorical_features])\n","        return self\n","    def transform(self, X):\n","        X_copy = X.copy()\n","        #Numerical features: Fill in missing values and scale\n","        X_copy[self.numeric_features] = self.num_scaler.transform(self.num_imputer.transform(X_copy[self.numeric_features]))\n","        #Category feature: Fill in the missing values\n","        X_copy[self.categorical_features] = self.cat_imputer.transform(X_copy[self.categorical_features])\n","        return X_copy\n","\n","\n","#Initialize the CatBoostRegressor model (baseline), and set the categorical features and random seed\n","baseline_model = CatBoostRegressor(cat_features=categorical_features, random_seed=42, verbose=0)\n","pipeline = Pipeline([\n","    (\"preprocessor\", DataFramePreprocessor(numeric_features, categorical_features)),\n","    (\"regressor\", baseline_model)\n","])\n"],"metadata":{"id":"QAn7h7ZIh-u1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#---------------------------------------------------------------     4.4.3 Five-fold cross-validation    --------------------------------------------------------------------\n","\n","#In order to evaluate the robustness of the model's performance on the training set,\n","#I will conduct 5-fold cross-validation and repeat the process multiple times to reduce the influence of random partitioning.\n","\n","\n","#Load saved X_reg, y_reg, X_test, y_test and feature lists\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","X_reg    = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg    = joblib.load(data_dir / 'y_reg.joblib')\n","X_test   = joblib.load(data_dir / 'X_test.joblib')\n","y_test   = joblib.load(data_dir / 'y_test.joblib')\n","with open(data_dir / 'feature_lists.json','r') as f:\n","    feat = json.load(f)\n","numeric_features     = feat['numeric']\n","categorical_features = feat['categorical']\n","\n","#Rebuild preprocessing pipelines\n","def clean_numeric_arr(arr):\n","    \"\"\"Strip non-numeric chars and cast to float.\"\"\"\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(\n","        lambda x: pd.to_numeric(re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce')\n","    ).values\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   FunctionTransformer(clean_numeric_arr, feature_names_out='one-to-one')),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","\n","categorical_pipe = Pipeline([\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","])\n","\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features),\n","])\n","\n","#Build CatBoost pipeline + CV ---\n","pipe = Pipeline([\n","    ('prep',  preprocessor),\n","    ('model', CatBoostRegressor(random_seed=42, verbose=0))\n","])\n","\n","cv = RepeatedKFold(n_splits=5, n_repeats=5, random_state=42)\n","scoring = {\n","    \"MAE\":  \"neg_mean_absolute_error\",\n","    \"RMSE\": \"neg_root_mean_squared_error\",\n","    \"R2\":   \"r2\"\n","}\n","\n","cv_results = cross_validate(\n","    pipe,\n","    X_reg, y_reg,\n","    cv=cv,\n","    scoring=scoring,\n","    n_jobs=1\n",")\n","\n","mae   = -cv_results['test_MAE']\n","rmse  = -cv_results['test_RMSE']\n","r2_sc =  cv_results['test_R2']\n","\n","print(f\"MAE:  {mae.mean():.3f} ± {mae.std():.3f}\")\n","print(f\"RMSE: {rmse.mean():.3f} ± {rmse.std():.3f}\")\n","print(f\"R2:   {r2_sc.mean():.3f} ± {r2_sc.std():.3f}\")\n"],"metadata":{"id":"16IZU3LwiJZz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#---------------------------------------------------     4.4.4 Test set predictions and saving of the CatBoost baseline model    --------------------------------------------------------------------\n","\n","#Reuse the preprocessor, numeric_features, categorical_features, X_reg, y_reg, X_test, and y_test that you have previously constructed.\n","feature_cols = numeric_features + categorical_features\n","X_test_reg   = X_test[feature_cols].copy()\n","\n","#Baseline CatBoost parameters\n","baseline_model = CatBoostRegressor(\n","    n_estimators=300,\n","    depth=6,\n","    learning_rate=0.1,\n","    l2_leaf_reg=3.0,\n","    loss_function='RMSE',\n","    random_seed=42,\n","    verbose=0,\n","    thread_count=-1\n",")\n","\n","pipeline = Pipeline([\n","    ('prep',  preprocessor),\n","    ('model', baseline_model),\n","])\n","pipeline.fit(X_reg, y_reg)\n","\n","#Test set evaluation\n","y_pred = pipeline.predict(X_test_reg)\n","mae  = mean_absolute_error(y_test, y_pred)\n","mse  = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2   = r2_score(y_test, y_pred)\n","print(f\"[Test] CatBoost baseline | MAE={mae:.3f} | RMSE={rmse:.3f} | R²={r2:.3f}\")\n","\n","#Save to google drive\n","base_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/catboost_baseline\")\n","base_dir.mkdir(parents=True, exist_ok=True)\n","\n","#Model（whole pipeline）\n","model_path = base_dir / \"catboost_baseline_pipeline.joblib\"\n","joblib.dump(pipeline, model_path, compress=3)\n","\n","#Test set metrics\n","metrics = {'model':'CatBoost','mae':float(mae),'rmse':float(rmse),'r2':float(r2)}\n","with open(base_dir / \"test_metrics_catboost.json\", \"w\") as f:\n","    json.dump(metrics, f, indent=2)\n","\n","#Test set prediction details\n","pred_df = pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_test)),\n","    'y_true': y_test.values,\n","    'y_pred': y_pred,\n","    'abs_error': np.abs(y_test.values - y_pred)\n","})\n","pred_df.to_csv(base_dir / \"test_predictions_catboost.csv\", index=False)\n","\n","#The cross-validation results are saved to facilitate visualization in the subsequent steps.\n","cv_df = pd.DataFrame({'MAE': -cv_results['test_MAE'], 'RMSE': -cv_results['test_RMSE'], 'R2': cv_results['test_R2']})\n","cv_df.to_csv(base_dir / \"cv_catboost.csv\", index=False)\n","cv_df[['MAE','RMSE','R2']].agg(['mean','std']).T.to_csv(base_dir / \"cv_catboost_summary.csv\")\n","\n","#Feature importance (mapping the feature names after OHE)\n","try:\n","    feat_names = pipeline.named_steps['prep'].get_feature_names_out()\n","    importances = pipeline.named_steps['model'].feature_importances_\n","    fi = (pd.DataFrame({'feature': feat_names, 'importance': importances})\n","          .sort_values('importance', ascending=False))\n","    fi.to_csv(base_dir / \"catboost_feature_importance.csv\", index=False)\n","except Exception as e:\n","    print(\"Skip feature importance:\", e)\n","\n","print(f\"Saved -> {model_path}\")\n"],"metadata":{"id":"pxhRP9EgiJoG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------     4.4.5 Using Optuna for automatic parameter tuning    --------------------------------------------------------------------\n","\n","\n","def objective(trial):\n","    params = {\n","        'n_estimators'       : trial.suggest_int('n_estimators', 150, 400),\n","        'depth'              : trial.suggest_int('depth', 4, 8),\n","        'learning_rate'      : trial.suggest_float('learning_rate', 0.02, 0.3, log=True),\n","        'l2_leaf_reg'        : trial.suggest_float('l2_leaf_reg', 1e-2, 10.0, log=True),\n","        'bagging_temperature': trial.suggest_float('bagging_temperature', 0.0, 1.0),\n","        'random_strength'    : trial.suggest_float('random_strength', 0.0, 2.0),\n","        # 固定项\n","        'loss_function' : 'RMSE',\n","        'verbose'       : 0,\n","        'random_seed'   : 42,\n","        'thread_count'  : -1\n","    }\n","    model = CatBoostRegressor(**params)\n","    pipe_tune = Pipeline([('prep', preprocessor), ('model', model)])\n","    #With the goal of achieving a neg RMSE of Five-fold cross-validation\n","    scores = cross_val_score(pipe_tune, X_reg, y_reg,\n","                             cv=5,\n","                             scoring='neg_root_mean_squared_error',\n","                             ## CatBoost has internal multi-threading, and the outer parallel processing is set to 1 to avoid excessive parallelism.\n","                             n_jobs=1)\n","    return -scores.mean()\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=50, show_progress_bar=True)\n","\n","print(\"Best params:\", study.best_params)\n"],"metadata":{"id":"XtA-SGeliJ1N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------     4.4.6 Refit best model & save    --------------------------------------------------------------------\n","\n","best_params = study.best_params.copy()\n","best_params.update({\n","    'loss_function':'RMSE',\n","    'verbose':0,\n","    'random_seed':42,\n","    'thread_count':-1\n","})\n","\n","best_model = CatBoostRegressor(**best_params)\n","best_pipe  = Pipeline([('prep', preprocessor), ('model', best_model)])\n","best_pipe.fit(X_reg, y_reg)\n","\n","#Test set evaluation\n","y_pred = best_pipe.predict(X_test_reg)\n","mae  = mean_absolute_error(y_test, y_pred)\n","mse  = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2   = r2_score(y_test, y_pred)\n","print(f\"[Test] CatBoost(Optuna) | MAE={mae:.3f} | RMSE={rmse:.3f} | R²={r2:.3f}\")\n","\n","#Save\n","opt_dir = Path(\"/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/catboost_optuna\")\n","opt_dir.mkdir(parents=True, exist_ok=True)\n","\n","joblib.dump(best_pipe, opt_dir / \"catboost_optuna_pipeline.joblib\", compress=3)\n","\n","with open(opt_dir / \"catboost_optuna_test_metrics.json\", \"w\") as f:\n","    json.dump({'mae':float(mae),'rmse':float(rmse),'r2':float(r2),\n","               'best_params': best_params}, f, indent=2)\n","\n","pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_test)),\n","    'y_true': y_test.values,\n","    'y_pred': y_pred,\n","    'abs_error': np.abs(y_test.values - y_pred)\n","}).to_csv(opt_dir / \"catboost_optuna_test_predictions.csv\", index=False)\n","\n","#All trial records of Optuna\n","try:\n","    study.trials_dataframe(attrs=('number','value','params','state')) \\\n","         .to_csv(opt_dir / \"optuna_trials_catboost.csv\", index=False)\n","except Exception:\n","    pass\n","\n","\n","try:\n","    feat_names = best_pipe.named_steps['prep'].get_feature_names_out()\n","    importances = best_pipe.named_steps['model'].feature_importances_\n","    pd.DataFrame({'feature': feat_names, 'importance': importances})\\\n","      .sort_values('importance', ascending=False)\\\n","      .to_csv(opt_dir / \"catboost_optuna_feature_importance.csv\", index=False)\n","except Exception as e:\n","    print(\"Skip feature importance:\", e)\n","\n","print(f\"Saved tuned pipeline -> {opt_dir/'catboost_optuna_pipeline.joblib'}\")\n"],"metadata":{"id":"rmYYG2cZiKCs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["###############################################################################################################################################################\n","\n","\n","#-------------------------------------------------    4.5 MLP (Multi-Layer Perceptron)    --------------------------------------------------------------------\n","\n","#-------------------------------------------------    4.5.1 Data Preprocessing Pipeline    --------------------------------------------------------------------\n","\n","# Load the prepared train/test data and feature lists from the previous section\n","\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","\n","##Load the training set and test set data.\n","X_reg  = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg  = joblib.load(data_dir / 'y_reg.joblib')\n","X_test = joblib.load(data_dir / 'X_test.joblib')\n","y_test = joblib.load(data_dir / 'y_test.joblib')\n","\n","print(\"Loaded saved dataset:\")\n","print(\"  • X_reg:\", X_reg.shape, \"  • y_reg:\", y_reg.shape)\n","print(\"  • X_test:\", X_test.shape, \"  • y_test:\", y_test.shape)\n","\n","# Identify numerical and categorical feature columns from X_reg\n","numeric_features = X_reg.select_dtypes(include=np.number).columns.tolist()\n","categorical_features = X_reg.select_dtypes(include=['object', 'category']).columns.tolist()\n","\n","# Define transformations for numeric and categorical features\n","numeric_transformer = Pipeline(steps=[\n","    ('scaler', StandardScaler())      # standardize numerical features\n","])\n","categorical_transformer = Pipeline(steps=[\n","    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # one-hot encode categorical features\n","])\n","\n","# Combine transformations into a ColumnTransformer\n","preprocessor = ColumnTransformer(transformers=[\n","    ('num', numeric_transformer, numeric_features),\n","    ('cat', categorical_transformer, categorical_features)\n","])"],"metadata":{"id":"oFC_MMQ6iKQp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------    4.5.2  MLPRegressor Model Setup with Early Stopping    --------------------------------------------------------------------\n","\n","#Define a default MLPRegressor (to be tuned later via Optuna), set early_stopping=True to enable early stopping.\n","default_mlp = MLPRegressor(hidden_layer_sizes=(64, 32),\n","                            activation='relu',\n","                            solver='adam',\n","                            learning_rate_init=0.01,\n","                            alpha=0.0001,\n","                            max_iter=300,\n","                            early_stopping=True,\n","                            n_iter_no_change=10,\n","                            random_state=42)\n"],"metadata":{"id":"_b5zQm3zhw-u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------    4.5.3  Hyperparameter Tuning with Optuna    --------------------------------------------------------------------\n","\n","\"\"\"\n","Use Optuna to perform hyperparameter optimization.\n","Define an Optuna objective function that trains an MLPRegressor pipeline with given hyperparameters and returns the 5-fold cross-validated MAE as the objective to minimize.\n","\"\"\"\n","\n","#Define the objective function for Optuna\n","def objective(trial):\n","    #Suggest values for hyperparameters\n","    n_layers = trial.suggest_int('n_layers', 1, 2)\n","    if n_layers == 1:\n","        #If one hidden layer, choose its size\n","        layer1 = trial.suggest_int('layer1_hidden_units', 32, 128, step=32)\n","        hidden_layers = (layer1,)\n","    else:\n","        #If two hidden layers, choose sizes for both\n","        layer1 = trial.suggest_int('layer1_hidden_units', 32, 128, step=32)\n","        layer2 = trial.suggest_int('layer2_hidden_units', 16, layer1, step=16)\n","        hidden_layers = (layer1, layer2)\n","    #Suggest alpha (L2 regularization) on a log scale for a wide range\n","    alpha = trial.suggest_float('alpha', 1e-5, 1e-1, log=True)\n","    #Suggest initial learning rate\n","    learning_rate = trial.suggest_float('learning_rate_init', 1e-4, 1e-1, log=True)\n","\n","    #Create an MLPRegressor with suggested hyperparameters\n","    mlp = MLPRegressor(hidden_layer_sizes=hidden_layers,\n","                       activation='relu',\n","                       solver='adam',\n","                       learning_rate_init=learning_rate,\n","                       alpha=alpha,\n","                       max_iter=300,\n","                       early_stopping=True,\n","                       n_iter_no_change=10,\n","                       random_state=42)\n","    #Build a pipeline with the preprocessor and the MLP regressor\n","    pipeline = Pipeline([\n","        ('preprocessor', preprocessor),\n","        ('regressor', mlp)\n","    ])\n","    #Perform 5-fold cross-validation (using MAE as the metric to minimize)\n","    cv_scores = cross_val_score(pipeline, X_reg, y_reg, cv=5, scoring='neg_mean_absolute_error')\n","    #Compute mean MAE (negative sign to convert from neg_mean_absolute_error to positive MAE)\n","    mean_mae = -cv_scores.mean()\n","    return mean_mae\n","\n","#Set up the Optuna study for minimization\n","study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n","study.optimize(objective, n_trials=50, timeout=None)  # You can adjust n_trials or set a timeout for the search\n","\n","#Output the best result from Optuna\n","print(f\"Best trial index: {study.best_trial.number}\")\n","print(f\"Best CV MAE: {study.best_value:.4f}\")\n","print(\"Best hyperparameters:\", study.best_params)\n"],"metadata":{"id":"6N9dzzrDk7KZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-------------------------------------------------    4.5.4  Cross-Validation Performance of the Best Model    --------------------------------------------------------------------\n","\n","#Retrieve the best hyperparameters from Optuna\n","best_params = study.best_params\n","\n","#Reconstruct the best hidden_layer_sizes tuple from best_params\n","if best_params.get('n_layers', 1) == 1:\n","    best_hidden_layers = (best_params['layer1_hidden_units'],)\n","else:\n","    best_hidden_layers = (best_params['layer1_hidden_units'], best_params['layer2_hidden_units'])\n","\n","#Create the MLPRegressor with best hyperparameters\n","best_mlp = MLPRegressor(hidden_layer_sizes=best_hidden_layers,\n","                        activation='relu',\n","                        solver='adam',\n","                        learning_rate_init=best_params['learning_rate_init'],\n","                        alpha=best_params['alpha'],\n","                        max_iter=300,\n","                        early_stopping=True,\n","                        n_iter_no_change=10,\n","                        random_state=42)\n","\n","#Create a pipeline with the preprocessor and best MLP\n","best_pipeline = Pipeline([\n","    ('preprocessor', preprocessor),\n","    ('regressor', best_mlp)\n","])\n","\n","#Evaluate with 5-fold cross-validation on training data using multiple metrics\n","cv_results = cross_validate(best_pipeline, X_reg, y_reg, cv=5,\n","                             scoring={'MAE': 'neg_mean_absolute_error',\n","                                      'RMSE': 'neg_root_mean_squared_error',\n","                                      'R2': 'r2'},\n","                             return_train_score=False)\n","#Calculate mean and std for each metric (note: negate the negatives for MAE and RMSE)\n","mae_scores = -cv_results['test_MAE']\n","rmse_scores = -cv_results['test_RMSE']\n","r2_scores = cv_results['test_R2']\n","\n","print(f\"5-Fold CV Results (Training set):\")\n","print(f\"MAE:  {mae_scores.mean():.3f} ± {mae_scores.std():.3f}\")\n","print(f\"RMSE: {rmse_scores.mean():.3f} ± {rmse_scores.std():.3f}\")\n","print(f\"R^2:  {r2_scores.mean():.3f} ± {r2_scores.std():.3f}\")\n","\n"],"metadata":{"id":"W3xRQHeJlAJE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#------------------------------------------------   4.5.5  Train Final Model on Full Training Data and Evaluate on Test Set    --------------------------------------------------------------------\n","\n","#Train the best pipeline on the full training data\n","best_pipeline.fit(X_reg, y_reg)\n","\n","#Make predictions on the test set\n","y_pred = best_pipeline.predict(X_test)\n","\n","#Create a DataFrame with true values, predictions, and absolute errors\n","results_df = pd.DataFrame({\n","    'y_true': y_test,\n","    'y_pred': y_pred,\n","    'abs_error': np.abs(y_test - y_pred)\n","})\n","print(\"Test set predictions with absolute errors:\")\n","print(results_df.head())  # Display the first few predictions for inspection\n","\n","#Ensure the target directory exists for saving outputs\n","output_dir = \"/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/mlp_model\"\n","os.makedirs(output_dir, exist_ok=True)\n","\n","#Save the results to a CSV file\n","results_df.to_csv(os.path.join(output_dir, \"mlp_optuna_predictions.csv\"), index=False)\n","\n","#Save the trained pipeline model to a joblib file\n","joblib.dump(best_pipeline, os.path.join(output_dir, \"mlp_optuna_pipeline.joblib\"))\n","\n","# Save Optuna tuning records and best parameters\n","# Save all trials results as CSV\n","optuna_results_df = study.trials_dataframe()\n","optuna_results_df.to_csv(os.path.join(output_dir, \"mlp_optuna_trials.csv\"), index=False)\n","#Save best hyperparameters to a text file\n","with open(os.path.join(output_dir, \"mlp_best_hyperparameters.txt\"), \"w\") as f:\n","    f.write(f\"Best Hyperparameters:\\n{study.best_params}\\n\")\n","    f.write(f\"\\nBest CV MAE: {study.best_value:.4f}\\n\")\n","\n","#Plot and save the optimization history (objective value vs. trial)\n","optuna.visualization.matplotlib.plot_optimization_history(study)\n","\n","plt.title(\"Optuna Optimization History\")\n","plt.savefig(os.path.join(output_dir, \"optuna_optimization_history.png\"))\n","plt.close()\n","\n","# Plot and save hyperparameter importances\n","optuna.visualization.matplotlib.plot_param_importances(study)\n","plt.title(\"Hyperparameter Importances\")\n","plt.savefig(os.path.join(output_dir, \"optuna_param_importances.png\"))\n","plt.close()\n"],"metadata":{"id":"3eNkdX-7lAgg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#------------------------------------------------   4.6  Optimize the random forest model again.    --------------------------------------------------------------------\n","\n","#------------------------------------------------   4.6.1  Optimize the random forest model   --------------------------------------------------------------------\n","\"\"\"\n","By comparing several models, it was found that the random forest performed the best. However, there is still a lot of room for improvement.\n","Therefore, it was decided to incorporate feature selection and optimization and run it again to see the results.\n","Random Forest + Feature Selection + Optuna\n","\"\"\"\n","\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","X_reg  = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg  = joblib.load(data_dir / 'y_reg.joblib')\n","X_test = joblib.load(data_dir / 'X_test.joblib')\n","y_test = joblib.load(data_dir / 'y_test.joblib')\n","with open(data_dir / 'feature_lists.json', 'r') as f:\n","    feat = json.load(f)\n","numeric_features     = feat['numeric']\n","categorical_features = feat['categororical'] if 'categororical' in feat else feat['categorical']\n","feature_cols         = numeric_features + categorical_features\n","\n","def clean_numeric_arr(arr):\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(\n","        lambda x: pd.to_numeric(re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce')\n","    ).values\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   FunctionTransformer(clean_numeric_arr, feature_names_out='one-to-one')),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","categorical_pipe = Pipeline([\n","    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","    ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","])\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features)\n","])\n","\n","#Build a pipeline of \"preprocessing -> feature selection -> RF\"\n","def build_pipeline(percentile:int,\n","                   n_estimators:int,\n","                   max_depth,\n","                   max_features,\n","                   min_samples_split:int,\n","                   min_samples_leaf:int,\n","                   bootstrap:bool):\n","    pipe = Pipeline([\n","        ('prep',  preprocessor),\n","        ('vt',    VarianceThreshold(threshold=0.0)),\n","        ('kbest', SelectPercentile(mutual_info_regression, percentile=percentile)),\n","        ('model', RandomForestRegressor(\n","            n_estimators=n_estimators,\n","            max_depth=max_depth,\n","            max_features=max_features,\n","            min_samples_split=min_samples_split,\n","            min_samples_leaf=min_samples_leaf,\n","            bootstrap=bootstrap,\n","            random_state=42,\n","            n_jobs=-1\n","        ))\n","    ])\n","    return pipe\n","\n","#Optuna objective function: 5-fold cross-validation, minimization of RMSE\n","cv = KFold(n_splits=5, shuffle=True, random_state=42)\n","\n","def objective(trial: optuna.trial.Trial):\n","    percentile        = trial.suggest_int('percentile', 40, 100, step=10)\n","    n_estimators      = trial.suggest_int('n_estimators', 300, 900, step=100)\n","    max_depth         = trial.suggest_categorical('max_depth', [None, 6, 8, 10, 12, 14, 16, 20])\n","    max_features      = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n","    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n","    min_samples_leaf  = trial.suggest_int('min_samples_leaf', 1, 5)\n","    bootstrap         = trial.suggest_categorical('bootstrap', [True, False])\n","\n","    pipe = build_pipeline(percentile, n_estimators, max_depth, max_features,\n","                          min_samples_split, min_samples_leaf, bootstrap)\n","\n","    scores = cross_val_score(pipe, X_reg[feature_cols], y_reg,\n","                             cv=cv, scoring='neg_root_mean_squared_error', n_jobs=-1)\n","    rmse_mean = -scores.mean()\n","    return rmse_mean\n","\n","study = optuna.create_study(direction='minimize')\n","study.optimize(objective, n_trials=50, show_progress_bar=True)\n","print(\"Best trial params:\", study.best_trial.params)\n","print(\"CV RMSE (best):\", study.best_value)\n"],"metadata":{"id":"6eqYDhs18AQ2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fit the entire training set with the optimal parameters and evaluate it on the test set.\n","bp = study.best_trial.params\n","best_pipe = build_pipeline(bp['percentile'], bp['n_estimators'], bp['max_depth'],\n","                           bp['max_features'], bp['min_samples_split'],\n","                           bp['min_samples_leaf'], bp['bootstrap'])\n","best_pipe.fit(X_reg[feature_cols], y_reg)\n","\n","y_pred = best_pipe.predict(X_test[feature_cols])\n","\n","mae = mean_absolute_error(y_test, y_pred)\n","mse = mean_squared_error(y_test, y_pred)\n","rmse = np.sqrt(mse)\n","r2  = r2_score(y_test, y_pred)\n","\n","print(f\"[Test] RF+FS(Optuna) | MAE={mae:.3f} | RMSE={rmse:.3f} | R²={r2:.3f}\")\n"],"metadata":{"id":"EtwX7kVD8Ad4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the output\n","out_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_fs_optuna')\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","#model\n","model_path = out_dir / 'rf_fs_optuna_pipeline.joblib'\n","joblib.dump(best_pipe, model_path, compress=3)\n","\n","#index\n","with open(out_dir / 'test_metrics_rf_fs.json', 'w') as f:\n","    json.dump({'model':'RF+FS(optuna)','mae':float(mae),'rmse':float(rmse),'r2':float(r2),'best_params':bp}, f, indent=2)\n","\n","pred_df = pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_test)),\n","    'y_true': np.asarray(y_test).ravel(),\n","    'y_pred': y_pred,\n","    'abs_error': np.abs(np.asarray(y_test).ravel() - y_pred)\n","})\n","pred_df.to_csv(out_dir / 'rf_fs_optuna_test_predictions.csv', index=False)\n","\n","#Optuna Experiment Log\n","try:\n","    study.trials_dataframe(attrs=('number','value','params','state')).to_csv(out_dir / 'optuna_trials_rf_fs.csv', index=False)\n","except Exception:\n","    pass\n","\n","#Selected features\n","try:\n","    feat_names = best_pipe.named_steps['prep'].get_feature_names_out()\n","    mask_vt    = best_pipe.named_steps['vt'].get_support()\n","    names_vt   = feat_names[mask_vt]\n","\n","    kbest      = best_pipe.named_steps['kbest']\n","    mask_k     = kbest.get_support()\n","    scores     = kbest.scores_\n","    selected_names  = names_vt[mask_k]\n","    selected_scores = scores[mask_k]\n","\n","    sel_df = (pd.DataFrame({'feature': selected_names,\n","                            'mutual_info': selected_scores})\n","                .sort_values('mutual_info', ascending=False)\n","                .reset_index(drop=True))\n","    sel_df.to_csv(out_dir / 'selected_features.csv', index=False)\n","except Exception as e:\n","    print(\"Skip saving selected features:\", e)\n","\n","print(f\"Saved pipeline -> {model_path}\")\n","print(f\"Saved metrics  -> {out_dir/'test_metrics_rf_fs.json'}\")\n","print(f\"Saved preds    -> {out_dir/'rf_fs_optuna_test_predictions.csv'}\")\n","print(f\"Saved features -> {out_dir/'selected_features.csv'}\")"],"metadata":{"id":"YzfMwzLB8Aqv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Computational regression + clinical hit rate\n","\n","pred_path = '/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_fs_optuna/rf_fs_optuna_test_predictions.csv'\n","df = pd.read_csv(pred_path)\n","\n","y_true = df['y_true'].to_numpy()\n","y_pred = df['y_pred'].to_numpy()\n","\n","#Core regression indicators\n","err     = y_pred - y_true\n","abs_err = np.abs(err)\n","\n","mae   = mean_absolute_error(y_true, y_pred)\n","rmse  = np.sqrt(mean_squared_error(y_true, y_pred))\n","r2    = r2_score(y_true, y_pred)\n","medae = median_absolute_error(y_true, y_pred)\n","bias  = np.mean(err)\n","\n","print(f\"MAE  = {mae:.3f} days\")\n","print(f\"RMSE = {rmse:.3f} days\")\n","print(f\"R²   = {r2:.3f}\")\n","print(f\"MedAE= {medae:.3f} days\")\n","print(f\"Bias = {bias:.3f} days  (Positive = Overall overestimation, Negative = Overall underestimation)\")\n","\n","#Clinical \"Hit Rate ± k Days\"\n","def coverage_within(k):\n","    return float(np.mean(abs_err <= k))\n","\n","for k in [0, 1, 2, 3]:\n","    print(f\"Hit Rate±{k}days = {coverage_within(k)*100:.1f}%\")\n","\n","#Perform bootstrapping for MAE/misfire rate with 95% confidence interval\n","def bootstrap_ci_on_mean(values, B=1000, seed=42):\n","    rng = np.random.default_rng(seed)\n","    n   = len(values)\n","    stats = []\n","    for _ in range(B):\n","        idx = rng.integers(0, n, n)\n","        stats.append(np.mean(values[idx]))\n","    lo, hi = np.percentile(stats, [2.5, 97.5])\n","    return lo, hi\n","\n","mae_lo, mae_hi = bootstrap_ci_on_mean(abs_err)\n","print(f\"MAE 95% CI ≈ [{mae_lo:.3f}, {mae_hi:.3f}] days\")\n","\n","#The hit rate for a certain k is given by CI\n","k = 1\n","hit = (abs_err <= k).astype(float)\n","lo, hi = bootstrap_ci_on_mean(hit)\n","print(f'Hit Rate±{k}days 95% CI ≈ [{lo*100:.1f}%, {hi*100:.1f}%]')\n"],"metadata":{"id":"wm8cnx2NixNm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#------------------------------------------------   4.6.2  Optimize the random forest model   --------------------------------------------------------------------\n","\n","\"\"\"\n","Next, based on the feature selection and Optuna hyperparameter tuning process in the previous regression model,\n","a random forest classifier will be constructed as a supplement.\n","\"\"\"\n","\n","#Read the file\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","X_reg  = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg  = joblib.load(data_dir / 'y_reg.joblib')\n","X_test = joblib.load(data_dir / 'X_test.joblib')\n","y_test = joblib.load(data_dir / 'y_test.joblib')\n","\n","with open(data_dir / 'feature_lists.json', 'r') as f:\n","    feat = json.load(f)\n","numeric_features     = feat['numeric']\n","categorical_features = feat['categororical'] if 'categororical' in feat else feat['categorical']\n","feature_cols         = numeric_features + categorical_features\n","\n","#A numerical cleaning function consistent with regression results\n","def clean_numeric_arr(arr):\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(\n","        lambda x: pd.to_numeric(re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce')\n","    ).values\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   FunctionTransformer(clean_numeric_arr, feature_names_out='one-to-one')),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","\n","#OneHotEncoder\n","try:\n","    categorical_pipe = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","except TypeError:\n","    categorical_pipe = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse=False))\n","    ])\n","\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features)\n","])\n","\n","#Generate binary classification labels based on the regression target\n"," #(using the median value from the training set as the threshold)\n","thr = float(np.median(np.asarray(y_reg).ravel()))\n","y_clf       = (np.asarray(y_reg).ravel()  > thr).astype(int)\n","y_clf_test  = (np.asarray(y_test).ravel() > thr).astype(int)\n","\n","#Classification pipeline (preprocessing -> variance filtering -> kbest -> RF classifier)\n","def build_pipeline_clf(percentile: int,\n","                       n_estimators: int,\n","                       max_depth,\n","                       max_features,\n","                       min_samples_split: int,\n","                       min_samples_leaf: int,\n","                       bootstrap: bool):\n","    pipe = Pipeline([\n","        ('prep',  preprocessor),\n","        ('vt',    VarianceThreshold(threshold=0.0)),\n","        ('kbest', SelectPercentile(mutual_info_classif, percentile=percentile)),\n","        ('model', RandomForestClassifier(\n","            n_estimators=n_estimators,\n","            max_depth=max_depth,\n","            max_features=max_features,\n","            min_samples_split=min_samples_split,\n","            min_samples_leaf=min_samples_leaf,\n","            bootstrap=bootstrap,\n","            random_state=42,\n","            n_jobs=-1\n","        ))\n","    ])\n","    return pipe\n","\n","#Optuna: Stratified 5-fold, maximize accuracy rate\n","cv_clf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","def objective_clf(trial: optuna.trial.Trial):\n","    percentile        = trial.suggest_int('percentile', 40, 100, step=10)\n","    n_estimators      = trial.suggest_int('n_estimators', 300, 900, step=100)\n","    max_depth         = trial.suggest_categorical('max_depth', [None, 6, 8, 10, 12, 14, 16, 20])\n","    max_features      = trial.suggest_categorical('max_features', ['sqrt', 'log2', None])\n","    min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n","    min_samples_leaf  = trial.suggest_int('min_samples_leaf', 1, 5)\n","    bootstrap         = trial.suggest_categorical('bootstrap', [True, False])\n","\n","    pipe = build_pipeline_clf(percentile, n_estimators, max_depth, max_features,\n","                              min_samples_split, min_samples_leaf, bootstrap)\n","    scores = cross_val_score(pipe, X_reg[feature_cols], y_clf,\n","                             cv=cv_clf, scoring='accuracy', n_jobs=-1)\n","    return scores.mean()\n","\n","study_clf = optuna.create_study(direction='maximize')\n","study_clf.optimize(objective_clf, n_trials=50, show_progress_bar=True)\n","print(\"Best trial params:\", study_clf.best_trial.params)\n","print(\"CV Accuracy (best):\", study_clf.best_value)\n"],"metadata":{"id":"DXBYne3YOgwi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Train the best model and evaluate it on the test set\n","bp = study_clf.best_trial.params\n","best_pipe_clf = build_pipeline_clf(bp['percentile'], bp['n_estimators'], bp['max_depth'], bp['max_features'], bp['min_samples_split'], bp['min_samples_leaf'], bp['bootstrap'])\n","\n","best_pipe_clf.fit(X_reg[feature_cols], y_clf)\n","\n","y_pred  = best_pipe_clf.predict(X_test[feature_cols])\n","y_proba = best_pipe_clf.predict_proba(X_test[feature_cols])\n","\n","acc        = accuracy_score(y_clf_test, y_pred)\n","f1_macro   = f1_score(y_clf_test, y_pred, average='macro')\n","f1_weight  = f1_score(y_clf_test, y_pred, average='weighted')\n","cm         = confusion_matrix(y_clf_test, y_pred)\n","\n","print(f\"\\n[Test] RF+FS(Optuna) Classification | \"\n","      f\"Accuracy={acc:.3f} | F1(macro)={f1_macro:.3f} | F1(weighted)={f1_weight:.3f}\")\n","print(\"\\nConfusion Matrix:\\n\", cm)\n","print(\"\\nClassification Report:\\n\", classification_report(y_clf_test, y_pred))\n"],"metadata":{"id":"7daq9amvOhnH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the model and output\n","out_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_fs_optuna_classification')\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","joblib.dump(best_pipe_clf, out_dir / 'rf_fs_optuna_clf_pipeline.joblib', compress=3)\n","\n","with open(out_dir / 'test_metrics_rf_fs_clf.json', 'w') as f:\n","    json.dump({\n","        'model': 'RF+FS(optuna)_classification',\n","        'threshold_train_median': thr,\n","        'accuracy': float(acc),\n","        'f1_macro': float(f1_macro),\n","        'f1_weighted': float(f1_weight),\n","        'best_params': bp\n","    }, f, indent=2)\n","\n","pred_df = pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_clf_test)),\n","    'y_true': y_clf_test,\n","    'y_pred': y_pred,\n","    'prob_class0': y_proba[:, 0],\n","    'prob_class1': y_proba[:, 1]\n","})\n","pred_df.to_csv(out_dir / 'rf_fs_optuna_clf_test_predictions.csv', index=False)\n","\n","#Confusion matrix\n","cm_df = pd.DataFrame(cm, index=['True_0','True_1'], columns=['Pred_0','Pred_1'])\n","cm_df.to_csv(out_dir / 'confusion_matrix.csv')\n","\n","print(f\"\\nSaved pipeline -> {out_dir/'rf_fs_optuna_clf_pipeline.joblib'}\")\n","print(f\"Saved metrics  -> {out_dir/'test_metrics_rf_fs_clf.json'}\")\n","print(f\"Saved preds    -> {out_dir/'rf_fs_optuna_clf_test_predictions.csv'}\")\n","print(f\"Saved CM       -> {out_dir/'confusion_matrix.csv'}\")"],"metadata":{"id":"nVqof2pYOh9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#------------------------------------------------   4.7  Logistic regression   --------------------------------------------------------------------\n","\"\"\"\n","Random forests perform well in regression problems, but the accuracy in classification problems needs to be improved. Overall, Logistic regression is suitable as a model for additional classification tasks.\n","We can further incorporate Logistic regression to enhance the classification performance.\n","\"\"\"\n","\n","#Discretize the consecutive LOS values into classification labels\n","#Define the LOS category: 0 = Short-term (<= 1 day), 1 = Medium-term (2 - 3 days), 2 = Long-term (>= 4 days)\n","def categorize_los(los_value):\n","    if los_value <= 1:\n","        return 0\n","    elif los_value <= 3:\n","        return 1\n","    else:\n","        return 2\n","\n","y_clf_train = np.array([categorize_los(v) for v in np.ravel(y_reg)])\n","y_clf_test  = np.array([categorize_los(v) for v in np.ravel(y_test)])\n","\n","\n","#Build a Logistic multi-class classification pipeline (with the same preprocessing and feature selection as regression/random forest classification)\n","def build_logreg_pipeline(percentile: int, C: float):\n","    pipe = Pipeline([\n","        ('prep',  preprocessor),\n","        ('vt',    VarianceThreshold(threshold=0.0)),\n","        ('kbest', SelectPercentile(mutual_info_classif, percentile=percentile)),\n","        ('model', LogisticRegression(\n","            max_iter=2000,\n","            multi_class='multinomial',\n","            solver='lbfgs',\n","            penalty='l2',\n","            C=C,\n","            class_weight='balanced',\n","            random_state=42\n","        ))\n","    ])\n","    return pipe\n","\n","#Optuna hyperparameter tuning: Select feature percentage & regularizer strength C, maximize Accuracy\n","cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","def objective_lgr(trial: optuna.trial.Trial):\n","    percentile = trial.suggest_int('percentile', 40, 100, step=10)\n","    C          = trial.suggest_float('C', 1e-3, 100, log=True)\n","\n","    pipe = build_logreg_pipeline(percentile, C)\n","\n","    scores = cross_val_score(\n","        pipe,\n","        X_reg[feature_cols], y_clf_train,\n","        cv=cv, scoring='accuracy', n_jobs=-1\n","    )\n","    return scores.mean()\n","\n","study_lgr = optuna.create_study(direction='maximize')\n","study_lgr.optimize(objective_lgr, n_trials=50, show_progress_bar=True)\n","\n","print(\"Best trial params (LogReg):\", study_lgr.best_trial.params)\n","print(\"CV Accuracy (best, LogReg):\", study_lgr.best_value)\n","\n"],"metadata":{"id":"KXJ1RZPhOkI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fit using the optimal parameters on the training set (X_reg), and evaluate on the independent test set (X_test)\n","bp = study_lgr.best_trial.params\n","best_lgr = build_logreg_pipeline(percentile=bp['percentile'], C=bp['C'])\n","best_lgr.fit(X_reg[feature_cols], y_clf_train)\n","\n","y_pred = best_lgr.predict(X_test[feature_cols])\n","y_proba = best_lgr.predict_proba(X_test[feature_cols])\n","\n","acc  = accuracy_score(y_clf_test, y_pred)\n","f1m  = f1_score(y_clf_test, y_pred, average='macro')\n","f1w  = f1_score(y_clf_test, y_pred, average='weighted')\n","cm   = confusion_matrix(y_clf_test, y_pred)\n","\n","print(f\"\\n[Test] Logistic+FS(Optuna) | Accuracy={acc:.3f} | F1(macro)={f1m:.3f} | F1(weighted)={f1w:.3f}\")\n","print(\"\\nConfusion Matrix:\")\n","print(cm)\n","\n","print(\"\\nClassification Report:\")\n","print(classification_report(y_clf_test, y_pred))\n"],"metadata":{"id":"f-6M1aeaOkZn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the model and output\n","out_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/logistic_fs_optuna_classification')\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","#Model\n","joblib.dump(best_lgr, out_dir / 'logreg_fs_optuna_clf_pipeline.joblib', compress=3)\n","\n","#Index\n","metrics = {\n","    'model': 'LogReg+FS(Optuna)',\n","    'accuracy': float(acc),\n","    'f1_macro': float(f1m),\n","    'f1_weighted': float(f1w),\n","    'best_params': bp\n","}\n","with open(out_dir / 'test_metrics_logreg_fs_clf.json', 'w') as f:\n","    json.dump(metrics, f, indent=2)\n","\n","#Prediction / Probability\n","pred_df = pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_clf_test)),\n","    'y_true': y_clf_test,\n","    'y_pred': y_pred\n","})\n","\n","for k in range(y_proba.shape[1]):\n","    pred_df[f'prob_class{k}'] = y_proba[:, k]\n","pred_df.to_csv(out_dir / 'logreg_fs_optuna_clf_test_predictions.csv', index=False)\n","\n","#Confusion matrix\n","cm_df = pd.DataFrame(cm, index=[f'True_{i}' for i in range(cm.shape[0])],\n","                        columns=[f'Pred_{i}' for i in range(cm.shape[1])])\n","cm_df.to_csv(out_dir / 'confusion_matrix_logreg.csv')\n","\n","print(f\"\\nSaved pipeline -> {out_dir/'logreg_fs_optuna_clf_pipeline.joblib'}\")\n","print(f\"Saved metrics  -> {out_dir/'test_metrics_logreg_fs_clf.json'}\")\n","print(f\"Saved preds    -> {out_dir/'logreg_fs_optuna_clf_test_predictions.csv'}\")\n","print(f\"Saved CM       -> {out_dir/'confusion_matrix_logreg.csv'}\")"],"metadata":{"id":"AwrPuzTtOkpr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#------------------------------------------------   4.8  Two- class model   --------------------------------------------------------------------\n","#Due to the slightly lower accuracy rate in the three-class model, it was decided to incorporate a two-class model for comparison.np.random.seed(42)\n","\n","#Read files\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","X_reg  = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg  = joblib.load(data_dir / 'y_reg.joblib')\n","X_test = joblib.load(data_dir / 'X_test.joblib')\n","y_test = joblib.load(data_dir / 'y_test.joblib')\n","\n","with open(data_dir / 'feature_lists.json', 'r') as f:\n","    feat = json.load(f)\n","numeric_features     = feat['numeric']\n","\n","categorical_features = feat['categororical'] if 'categororical' in feat else feat['categorical']\n","feature_cols         = numeric_features + categorical_features\n","\n","def clean_numeric_arr(arr):\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(\n","        lambda x: pd.to_numeric(re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce')\n","    ).values\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   FunctionTransformer(clean_numeric_arr, feature_names_out='one-to-one')),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","\n","try:\n","    categorical_pipe = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","except TypeError:\n","    categorical_pipe = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse=False))\n","    ])\n","\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features)\n","])\n","\n","#Generate classification labels\n","LONG_STAY_DAYS = 3\n","y_clf      = (np.asarray(y_reg).ravel()  >= LONG_STAY_DAYS).astype(int)\n","y_clf_test = (np.asarray(y_test).ravel() >= LONG_STAY_DAYS).astype(int)\n","\n","#Hierarchical cross-validation object (classification)\n","cv_clf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","print(f\"Train size={len(y_reg)}, Test size={len(y_test)}\")\n","print(\"Train positive rate:\", y_clf.mean(), \" | Test positive rate:\", y_clf_test.mean())"],"metadata":{"id":"X-sV_riNUhnR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Scan the threshold on the validation set and maximize F1\n","def find_best_threshold(y_true, y_proba, average='weighted'):\n","    best_thr, best_f1 = 0.50, -1.0\n","    for thr in np.linspace(0.1, 0.9, 81):\n","        y_hat = (y_proba >= thr).astype(int)\n","        f1 = f1_score(y_true, y_hat, average=average, zero_division=0)\n","        if f1 > best_f1:\n","            best_f1, best_thr = f1, thr\n","    return best_thr, best_f1\n","\n","#Preprocessing consistent with regression + variance filtering + mutual information selection + model\n","def build_rf_clf(percentile, n_estimators, max_depth, max_features,\n","                 min_samples_split, min_samples_leaf, bootstrap):\n","    return Pipeline([\n","        ('prep',  preprocessor),\n","        ('vt',    VarianceThreshold(0.0)),\n","        ('kbest', SelectPercentile(mutual_info_classif, percentile=percentile)),\n","        ('model', RandomForestClassifier(\n","            n_estimators=n_estimators, max_depth=max_depth, max_features=max_features,\n","            min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n","            bootstrap=bootstrap, class_weight='balanced', random_state=42, n_jobs=-1\n","        ))\n","    ])\n","\n","def build_logreg_clf(percentile, C):\n","    return Pipeline([\n","        ('prep',  preprocessor),\n","        ('vt',    VarianceThreshold(0.0)),\n","        ('kbest', SelectPercentile(mutual_info_classif, percentile=percentile)),\n","        ('model', LogisticRegression(\n","            C=C, penalty='l2', solver='lbfgs', max_iter=2000,\n","            class_weight='balanced', n_jobs=-1\n","        ))\n","    ])"],"metadata":{"id":"psBxKtQ_Uh8W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","#Optuna hyperparameter tuning (objective: weighted-F1)\n","cv_clf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n","\n","def objective_rf(trial):\n","    params = dict(\n","        percentile       = trial.suggest_int('percentile', 50, 100, step=10),\n","        n_estimators     = trial.suggest_int('n_estimators', 300, 900, step=100),\n","        max_depth        = trial.suggest_categorical('max_depth', [None, 8, 12, 16, 20]),\n","        max_features     = trial.suggest_categorical('max_features', ['sqrt', 'log2', None]),\n","        min_samples_split= trial.suggest_int('min_samples_split', 2, 10),\n","        min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5),\n","        bootstrap        = trial.suggest_categorical('bootstrap', [True, False])\n","    )\n","    pipe = build_rf_clf(**params)\n","    scores = cross_val_score(pipe, X_reg[feature_cols], y_clf,\n","                             cv=cv_clf, scoring='f1_weighted', n_jobs=-1)\n","    return scores.mean()\n","\n","def objective_lr(trial):\n","    params = dict(\n","        percentile=trial.suggest_int('percentile', 50, 100, step=10),\n","        C=trial.suggest_float('C', 0.05, 5.0, log=True)\n","    )\n","    pipe = build_logreg_clf(**params)\n","    scores = cross_val_score(pipe, X_reg[feature_cols], y_clf,\n","                             cv=cv_clf, scoring='f1_weighted', n_jobs=-1)\n","    return scores.mean()\n","\n","print(\"\\n=== Optuna: RandomForestClassifier (long-stay >=3d) ===\")\n","study_rf = optuna.create_study(direction='maximize')\n","study_rf.optimize(objective_rf, n_trials=40, show_progress_bar=True)\n","print(\"Best RF params:\", study_rf.best_trial.params, \" | CV F1(weighted):\", study_rf.best_value)\n","\n","print(\"\\n=== Optuna: LogisticRegression (long-stay >=3d) ===\")\n","study_lr = optuna.create_study(direction='maximize')\n","study_lr.optimize(objective_lr, n_trials=40, show_progress_bar=True)\n","print(\"Best LR params:\", study_lr.best_trial.params, \" | CV F1(weighted):\", study_lr.best_value)"],"metadata":{"id":"KGnGZvn7UiKh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Extract a portion of the training set as the validation set to adjust the probability threshold\n","X_tr, X_val, y_tr, y_val = train_test_split(\n","    X_reg[feature_cols], y_clf, test_size=0.2, random_state=42, stratify=y_clf\n",")\n","\n","# ==== RF ====\n","rf_best = build_rf_clf(**study_rf.best_trial.params).fit(X_tr, y_tr)\n","val_proba = rf_best.predict_proba(X_val)[:, 1]\n","best_thr, best_val_f1 = find_best_threshold(y_val, val_proba, average='weighted')\n","print(f\"\\n[RF] best threshold on valid = {best_thr:.2f} | F1(weighted-valid) = {best_val_f1:.3f}\")\n","\n","test_proba = rf_best.predict_proba(X_test[feature_cols])[:, 1]\n","y_pred_thr = (test_proba >= best_thr).astype(int)\n","\n","acc  = accuracy_score(y_clf_test, y_pred_thr)\n","f1m  = f1_score(y_clf_test, y_pred_thr, average='macro',    zero_division=0)\n","f1w  = f1_score(y_clf_test, y_pred_thr, average='weighted', zero_division=0)\n","auc  = roc_auc_score(y_clf_test, test_proba)\n","cm   = confusion_matrix(y_clf_test, y_pred_thr)\n","print(f\"[Test] RF(long-stay) | Acc={acc:.3f} | F1(macro)={f1m:.3f} | F1(weighted)={f1w:.3f} | ROC-AUC={auc:.3f}\")\n","print(\"\\nConfusion Matrix:\\n\", cm)\n","print(\"\\nClassification Report:\\n\", classification_report(y_clf_test, y_pred_thr, digits=2))\n"],"metadata":{"id":"QVsLFrkPUiZw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the result of RF\n","out_rf = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/rf_longstay_binary')\n","out_rf.mkdir(parents=True, exist_ok=True)\n","joblib.dump(rf_best, out_rf/'rf_longstay_bin_pipeline.joblib', compress=3)\n","pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_clf_test)),\n","    'y_true': y_clf_test, 'y_pred': y_pred_thr, 'proba_pos': test_proba\n","}).to_csv(out_rf/'rf_longstay_bin_test_predictions.csv', index=False)\n","pd.DataFrame(cm, index=['True_0','True_1'], columns=['Pred_0','Pred_1']).to_csv(out_rf/'confusion_matrix.csv')\n","with open(out_rf/'best_params.json','w') as f:\n","    json.dump({'best_params': study_rf.best_trial.params, 'best_thr': float(best_thr)}, f, indent=2)\n"],"metadata":{"id":"86FKtVKbUinW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#------------Logistic Regression---------------\n","lr_best = build_logreg_clf(**study_lr.best_trial.params).fit(X_tr, y_tr)\n","val_proba_lr = lr_best.predict_proba(X_val)[:, 1]\n","best_thr_lr, best_val_f1_lr = find_best_threshold(y_val, val_proba_lr, average='weighted')\n","print(f\"\\n[LogReg] best threshold on valid = {best_thr_lr:.2f} | F1(weighted-valid) = {best_val_f1_lr:.3f}\")\n","\n","test_proba_lr = lr_best.predict_proba(X_test[feature_cols])[:, 1]\n","y_pred_thr_lr = (test_proba_lr >= best_thr_lr).astype(int)\n","\n","acc  = accuracy_score(y_clf_test, y_pred_thr_lr)\n","f1m  = f1_score(y_clf_test, y_pred_thr_lr, average='macro',    zero_division=0)\n","f1w  = f1_score(y_clf_test, y_pred_thr_lr, average='weighted', zero_division=0)\n","auc  = roc_auc_score(y_clf_test, test_proba_lr)\n","cm   = confusion_matrix(y_clf_test, y_pred_thr_lr)\n","print(f\"[Test] LogReg(long-stay) | Acc={acc:.3f} | F1(macro)={f1m:.3f} | F1(weighted)={f1w:.3f} | ROC-AUC={auc:.3f}\")\n","print(\"\\nConfusion Matrix:\\n\", cm)\n","print(\"\\nClassification Report:\\n\", classification_report(y_clf_test, y_pred_thr_lr, digits=2))\n"],"metadata":{"id":"GN1K9XVZUi08"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Save the result of LogReg\n","out_lr = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/logreg_longstay_binary')\n","out_lr.mkdir(parents=True, exist_ok=True)\n","joblib.dump(lr_best, out_lr/'logreg_longstay_bin_pipeline.joblib', compress=3)\n","pd.DataFrame({\n","    'ID': X_test['ID'].values if 'ID' in X_test.columns else np.arange(len(y_clf_test)),\n","    'y_true': y_clf_test, 'y_pred': y_pred_thr_lr, 'proba_pos': test_proba_lr\n","}).to_csv(out_lr/'logreg_longstay_bin_test_predictions.csv', index=False)\n","pd.DataFrame(cm, index=['True_0','True_1'], columns=['Pred_0','Pred_1']).to_csv(out_lr/'confusion_matrix.csv')\n","with open(out_lr/'best_params.json','w') as f:\n","    json.dump({'best_params': study_lr.best_trial.params, 'best_thr': float(best_thr_lr)}, f, indent=2)"],"metadata":{"id":"r8ZDqrY7UjEb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"labrIfgNOlyx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["########################################################################################################################################################\n","\"\"\"\n","Fifth part\n","Explanatory Analysis and Visualization\n","After comparison and consideration, it was decided to select the Random Forest + Feature Selection + Optuna model as the final model.\n","\n","\"\"\"\n","\n","#-------------------------------------------------    5.1 Permutation Importance on the final locked model    --------------------------------------------------------------------\n","\n","def clean_numeric_arr(arr):\n","    \"\"\"Strip non-numeric chars and cast to float.\"\"\"\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(\n","        lambda x: pd.to_numeric(re.sub(r'[^0-9\\.\\-]+', '', str(x)), errors='coerce')\n","    ).values\n","\n","model_name = 'random_forest_fs_optuna'\n","final_model_path = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_fs_optuna/rf_fs_optuna_pipeline.joblib')\n","\n","#save interpretability outputs\n","out_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/interpretability') / model_name\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","#Load held-out test set + feature lists\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","\n","# Keep for compatibility (X_reg/y_reg unused here)\n","X_reg  = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg  = joblib.load(data_dir / 'y_reg.joblib')\n","X_test = joblib.load(data_dir / 'X_test.joblib')\n","y_test = joblib.load(data_dir / 'y_test.joblib')\n","\n","with open(data_dir / 'feature_lists.json', 'r') as f:\n","    feat = json.load(f)\n","numeric_features     = feat['numeric']\n","categorical_features = feat['categorical']\n","feature_cols         = numeric_features + categorical_features\n","\n","#Ensure pass only expected raw columns\n","X_test_reg = X_test[feature_cols].copy()\n","\n","print(f\"Loaded test set for PI: X_test_reg={X_test_reg.shape}, y_test={y_test.shape}\")\n","print(f\"numeric={len(numeric_features)}, categorical={len(categorical_features)}\")\n","\n","# Load the final pipeline (preprocessor + RandomForestRegressor)\n","final_pipe = joblib.load(final_model_path)\n","print(f\"Loaded final pipeline: {final_model_path}\")\n","\n"],"metadata":{"id":"UflxSWyplAxK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","# Define scorers for PI\n","# permutation_importance assumes 'higher is better'.\n","# For RMSE/MAE we set greater_is_better=False so the scorer returns negative values.\n","\"\"\"\n","\n","def rmse(y_true, y_pred):\n","    return np.sqrt(mean_squared_error(y_true, y_pred))\n","\n","scorer_r2   = make_scorer(r2_score)\n","scorer_rmse = make_scorer(rmse, greater_is_better=False)\n","scorer_mae  = make_scorer(mean_absolute_error, greater_is_better=False)\n","\n","scorers = {\n","    'R2': scorer_r2,\n","    'RMSE': scorer_rmse,\n","    'MAE': scorer_mae\n","}\n","\n","#Run permutation importance for each metric\n","n_repeats    = 50\n","random_state = 42\n","\n","all_tables = []\n","for metric_name, scorer in scorers.items():\n","    print(f\"\\n>>> Computing permutation importance by {metric_name} ...\")\n","    pi = permutation_importance(\n","        estimator=final_pipe,\n","        X=X_test_reg,\n","        y=y_test,\n","        scoring=scorer,\n","        n_repeats=n_repeats,\n","        random_state=random_state,\n","        n_jobs=-1\n","    )\n","    #Build tidy table\n","    pi_df = (pd.DataFrame({\n","                'feature': feature_cols,\n","                'importance_mean': pi.importances_mean,\n","                'importance_std':  pi.importances_std\n","            })\n","             .sort_values('importance_mean', ascending=False)\n","             .reset_index(drop=True))\n","    pi_df['metric'] = metric_name\n","    all_tables.append(pi_df)\n","\n","#Concatenate results across metrics\n","pi_all = pd.concat(all_tables, axis=0, ignore_index=True)\n","\n","#Also provide a wide-format summary for quick comparison\n","pi_wide = (pi_all\n","           .pivot_table(index='feature', columns='metric', values='importance_mean')\n","           .sort_values(by=['R2'], ascending=False))\n","\n","#Save CSVs\n","csv_long  = out_dir / f'permutation_importance_{model_name}.csv'\n","csv_wide  = out_dir / f'permutation_importance_{model_name}_wide.csv'\n","pi_all.to_csv(csv_long, index=False)\n","pi_wide.to_csv(csv_wide)\n","print(f\"Saved PI (long)  -> {csv_long}\")\n","print(f\"Saved PI (wide)  -> {csv_wide}\")\n","\n"],"metadata":{"id":"BkkICAsDlA-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot top-N bar charts per metric\n","top_n = 20\n","for metric_name in scorers.keys():\n","    sub = (pi_all[pi_all['metric']==metric_name]\n","           .sort_values('importance_mean', ascending=False)\n","           .head(top_n))\n","    plt.figure(figsize=(8, 6))\n","    #Horizontal bar chart (sorted small->large so the biggest is at the top after reversing)\n","    plt.barh(sub['feature'][::-1], sub['importance_mean'][::-1])\n","    plt.xlabel(f'Permutation Importance ({metric_name})')\n","    plt.ylabel('Feature')\n","    plt.title(f'Permutation Importance (Top {top_n}) - {model_name} - {metric_name}')\n","    plt.tight_layout()\n","    png_path = out_dir / f'permutation_importance_{model_name}_{metric_name}.png'\n","    plt.savefig(png_path, dpi=200)\n","    plt.close()\n","    print(f\"Saved PI barplot ({metric_name}) -> {png_path}\")\n","\n","#Also export a Top-N pretty table for quick viewing\n","pretty_path = out_dir / f'permutation_importance_{model_name}_top{top_n}.csv'\n","(pd.concat([\n","    (pi_all[pi_all['metric']==m]\n","     .sort_values('importance_mean', ascending=False)\n","     .head(top_n)\n","     .assign(metric=m))\n","    for m in scorers.keys()\n","], ignore_index=True)\n",").to_csv(pretty_path, index=False)\n","print(f\"Saved Top-{top_n} per metric -> {pretty_path}\")\n"],"metadata":{"id":"zIGUvsDulBK6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------    5.2 SHAP TreeExplainer     --------------------------------------------------------------------\n","\n","plt.rcParams[\"figure.dpi\"] = 150\n","\n","rf_name = 'random_forest_fs_optuna'\n","rf_path = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_fs_optuna/rf_fs_optuna_pipeline.joblib')\n","\n","#Also explain XGB for appendix and compare\n","also_run_xgb = True\n","xgb_name = 'xgboost_optuna'\n","xgb_path = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/xgboost_optuna/xgb_optuna_pipeline.joblib')\n","\n","#Shared data (train/test & feature lists)\n","data_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines')\n","#Base output dir\n","base_out = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/interpretability')\n","base_out.mkdir(parents=True, exist_ok=True)\n","\n","def _to_dense(X):\n","    return X.toarray() if hasattr(X, \"toarray\") else X\n","\n","#Pass the original feature DataFrame through prep -> vt (optional) -> kbest (optional) successively,\n","#and return the 【matrix used for the model】 and its 【corresponding feature names】.\n","def _transform_to_model_space(pipe, X_raw_df):\n","    steps = pipe.named_steps\n","\n","    #1) prep\n","    X = steps['prep'].transform(X_raw_df)\n","    X = _to_dense(X)\n","    names = steps['prep'].get_feature_names_out()\n","\n","    #2) vt\n","    if 'vt' in steps:\n","        mask_vt = steps['vt'].get_support()\n","        X = steps['vt'].transform(X)\n","        names = names[mask_vt]\n","\n","    #3) kbest\n","    if 'kbest' in steps:\n","        mask_k = steps['kbest'].get_support()\n","        X = steps['kbest'].transform(X)\n","        names = names[mask_k]\n","\n","    return X, names\n","\n","def run_shap_for_pipeline(model_name: str, model_path: Path):\n","    print(f\"\\n------------------- SHAP for {model_name} --------------------\\n\")\n","\n","    out_dir = base_out / model_name / 'shap'\n","    out_dir.mkdir(parents=True, exist_ok=True)\n","\n","    #Loading data and columns\n","    X_reg  = joblib.load(data_dir / 'X_reg.joblib')\n","    y_reg  = joblib.load(data_dir / 'y_reg.joblib')\n","    X_test = joblib.load(data_dir / 'X_test.joblib')\n","    y_test = joblib.load(data_dir / 'y_test.joblib')\n","    with open(data_dir / 'feature_lists.json', 'r') as f:\n","        feat_json = json.load(f)\n","    feature_cols = feat_json['numeric'] + feat_json['categorical']\n","\n","    #Original column (before entering the pipeline)\n","    X_train_raw = X_reg[feature_cols].copy()\n","    X_test_raw  = X_test[feature_cols].copy()\n","\n","    #Load the complete pipeline\n","    pipe  = joblib.load(model_path)\n","    model = pipe.named_steps['model']\n","\n","    #Transform the training/test data into the \"model space\" (after going through prep/vt/kbest)\n","    X_bg_arr, final_feat_names = _transform_to_model_space(pipe, X_train_raw)\n","    X_t_arr,  _                = _transform_to_model_space(pipe, X_test_raw)\n","\n","    X_bg_df = pd.DataFrame(X_bg_arr, columns=final_feat_names)\n","    X_t_df  = pd.DataFrame(X_t_arr,  columns=final_feat_names).reset_index(drop=True)\n","\n","    #Downsampling the background to speed up\n","    bg_n = min(200, len(X_bg_df))\n","    X_bg_df = X_bg_df.sample(n=bg_n, random_state=42)\n","\n","    #Build and calculate SHAP (the tree model will automatically use TreeExplainer)\n","    explainer   = shap.Explainer(model, X_bg_df, feature_names=final_feat_names)\n","    explanation = explainer(X_t_df)\n","\n","    #Global Visualization\n","    plt.figure(figsize=(8, 6))\n","    shap.plots.beeswarm(explanation, max_display=20, show=False)\n","    plt.title(f\"SHAP Beeswarm (Top-20) - {model_name}\")\n","    plt.tight_layout()\n","    plt.savefig(out_dir / f'shap_beeswarm_top20_{model_name}.png')\n","    plt.close()\n","\n","    plt.figure(figsize=(8, 6))\n","    shap.plots.bar(explanation, max_display=20, show=False)\n","    plt.title(f\"SHAP Global Importance (mean |SHAP|) - {model_name}\")\n","    plt.tight_layout()\n","    plt.savefig(out_dir / f'shap_bar_top20_{model_name}.png')\n","    plt.close()\n","\n","    #Output global ranking CSV\n","    shap_abs_mean = np.abs(explanation.values).mean(axis=0)\n","    pd.DataFrame({'feature': final_feat_names, 'mean_abs_shap': shap_abs_mean})\\\n","      .sort_values('mean_abs_shap', ascending=False)\\\n","      .reset_index(drop=True)\\\n","      .to_csv(out_dir / f'shap_importance_{model_name}.csv', index=False)\n","\n","    #Select 3 to 5 typical cases and provide local explanations\n","    y_np   = y_test.to_numpy() if hasattr(y_test, 'to_numpy') else np.asarray(y_test)\n","    y_pred = pipe.predict(X_test_raw)\n","    abs_err = np.abs(y_np - y_pred)\n","\n","    order    = np.argsort(abs_err)\n","    best_i   = int(order[0])\n","    median_i = int(order[len(order)//2])\n","    worst_i  = int(order[-1])\n","    q1_i     = int(order[len(order)//4])\n","    q3_i     = int(order[int(0.75*len(order))])\n","    selected = [('best', best_i), ('q1', q1_i), ('median', median_i), ('q3', q3_i), ('worst', worst_i)]\n","\n","    sel_df = pd.DataFrame({\n","        'tag'      : [t for t, _ in selected],\n","        'pos_idx'  : [i for _, i in selected],\n","        'y_true'   : [float(y_np[i])   for _, i in selected],\n","        'y_pred'   : [float(y_pred[i]) for _, i in selected],\n","        'abs_error': [float(abs_err[i]) for _, i in selected],\n","    })\n","    sel_df.to_csv(out_dir / f'shap_selected_cases_{model_name}.csv', index=False)\n","\n","    for tag, pos_idx in selected:\n","        row_df = X_t_df.iloc[[pos_idx]]\n","        exp_i  = explainer(row_df)\n","\n","        #Waterfall plot\n","        plt.figure(figsize=(8, 6))\n","        shap.plots.waterfall(exp_i[0], max_display=20, show=False)\n","        plt.title(f\"Waterfall - {model_name} | case={tag} (pos={pos_idx}) | \"\n","                  f\"y={sel_df.loc[sel_df.tag==tag, 'y_true'].values[0]:.3f}, \"\n","                  f\"yhat={sel_df.loc[sel_df.tag==tag, 'y_pred'].values[0]:.3f}\")\n","        plt.tight_layout()\n","        plt.savefig(out_dir / f'waterfall_{model_name}_{tag}_pos{pos_idx}.png')\n","        plt.close()\n","\n","        #Force-directed graph (HTML)\n","        fp = shap.force_plot(\n","            base_value=exp_i.base_values[0],\n","            shap_values=exp_i.values[0],\n","            features=row_df.iloc[0, :],\n","            matplotlib=False\n","        )\n","        shap.save_html(str(out_dir / f'force_{model_name}_{tag}_pos{pos_idx}.html'), fp)\n","\n","    print(f\"\\nSaved SHAP results -> {out_dir}\\n\")\n","\n","#Run the final RF + FS model\n","run_shap_for_pipeline(rf_name, rf_path)\n","\n"],"metadata":{"id":"NWlRGjKalBvd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------    5.3 Partial Dependence / ICE     --------------------------------------------------------------------\n","\n","plt.rcParams[\"figure.dpi\"] = 150\n","\n","model_name = \"random_forest_fs_optuna\"\n","model_path = Path(\"/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_fs_optuna/rf_fs_optuna_pipeline.joblib\")\n","data_dir   = Path(\"/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/regression_baselines\")\n","out_dir    = Path(\"/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/interpretability\") / model_name / \"pdp_ice\"\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","#Loading data and features\n","X_reg  = joblib.load(data_dir / \"X_reg.joblib\")\n","X_test = joblib.load(data_dir / \"X_test.joblib\")\n","with open(data_dir / \"feature_lists.json\", \"r\") as f:\n","    feat = json.load(f)\n","numeric_features     = feat[\"numeric\"]\n","categorical_features = feat[\"categorical\"]\n","feature_cols         = numeric_features + categorical_features\n","\n","#Uniformly use the training set\n","X_used = X_reg[feature_cols].copy()\n","\n","#Load final pipeline\n","pipe = joblib.load(model_path)\n","print(\"Loaded final pipeline ->\", model_path)\n","\n","feat_1d = [\n","    \"Age (years):\",\n","    \"BMI\",\n","    \"How anxious are you\",\n","    \"How long do you expect your recovery after your operation?\",\n","    \"How long do you expect to use aids to walk?\",\n","    \"How long do you estimate you can get to a chair again?\"\n","]\n","\n","#2D Interaction\n","feat_2d = [\n","    (\"Age (years):\", \"PersonGender\"),\n","    (\"Age (years):\", \"Do you live alone?\"),\n","    (\"Age (years):\", \"How anxious are you\"),\n","]\n","\n","smooth_window = 5\n","\n","#Small tool: Recalculate the average PDP again outside of from_estimator to achieve smoothing.\n","def compute_avg_pdp(estimator, X, feature, grid_resolution=30):\n","    pd_result = partial_dependence(\n","        estimator=estimator,\n","        X=X,\n","        features=[feature],\n","        kind=\"average\",\n","        grid_resolution=grid_resolution\n","    )\n","    grid_x = pd_result[\"values\"][0]\n","    avg    = pd_result[\"average\"][0]\n","    return grid_x, avg\n"],"metadata":{"id":"m_SSAggfgZbk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["smooth_window = globals().get(\"smooth_window\", 5)\n","\n","#PDP calculation function compatible with all sklearn versions\n","def compute_avg_pdp(estimator, X, feature, grid_resolution=50):\n","    \"\"\"\n","    Returns (grid_x, avg_pdp_1d). Compatible with sklearn's different return formats:\n","      - New version: dict containing 'grid_values' + 'average'\n","      - Newer: dict containing 'values' + 'average'\n","      - Old version: tuple (avg, values)\n","    \"\"\"\n","    res = partial_dependence(\n","        estimator=estimator,\n","        X=X,\n","        features=[feature],\n","        grid_resolution=grid_resolution,\n","        kind=\"average\"\n","    )\n","\n","    # New/newer version: dict-like\n","    if isinstance(res, dict) or hasattr(res, \"__getitem__\"):\n","        if 'grid_values' in res:\n","            grid_x = np.asarray(res['grid_values'][0])\n","            avg    = np.asarray(res['average'][0])\n","            return grid_x, avg\n","        if 'values' in res:\n","            grid_x = np.asarray(res['values'][0])\n","            avg    = np.asarray(res['average'][0])\n","            return grid_x, avg\n","\n","    # Old version: directly returns (avg, values)\n","    try:\n","        avg, values = res\n","        grid_x = np.asarray(values[0])\n","        avg    = np.asarray(avg[0])\n","        return grid_x, avg\n","    except Exception as e:\n","        raise RuntimeError(\n","            f\"Unsupported sklearn partial_dependence return type: {type(res)}\"\n","        ) from e\n","\n","#Prepare a \"numeric\" X_pdp for PDP (only convert needed continuous variables)\n","X_pdp = X_used.copy()\n","\n","cont_cols = set(list(feat_1d) + [c for pair in feat_2d for c in pair])\n","\n","def _coerce_float_col(df, col):\n","    if col not in df.columns:\n","        return\n","    # If object/string column, remove non-numeric characters first then convert to float\n","    if df[col].dtype == 'O':\n","        s = df[col].astype(str).str.replace(r'[^0-9.\\-]+', '', regex=True)\n","        df[col] = pd.to_numeric(s, errors='coerce')\n","    # For numeric columns, fill NA with median to avoid percentile errors\n","    if np.issubdtype(df[col].dtype, np.number):\n","        df[col] = df[col].fillna(df[col].median())\n","\n","for c in cont_cols:\n","    _coerce_float_col(X_pdp, c)\n","\n","#3) Single-variable PDP + ICE\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","for f in feat_1d:\n","    if f not in X_used.columns:\n","        print(f\"[Skip] {f} not in columns.\")\n","        continue\n","\n","    fig, ax = plt.subplots(figsize=(6, 4))\n","    PartialDependenceDisplay.from_estimator(\n","        estimator=pipe,\n","        X=X_pdp,\n","        features=[f],\n","        kind=\"both\",\n","        centered=True,\n","        grid_resolution=30,\n","        ax=ax,\n","        ice_lines_kw={\"alpha\": 0.15, \"linewidth\": 0.8},\n","        pd_line_kw={\"linewidth\": 2}\n","    )\n","    ax.set_title(f\"PDP + ICE — {model_name} — {f}\")\n","\n","    #Add smoothed curve (rolling average)\n","    if smooth_window and smooth_window > 1:\n","        grid_x, avg = compute_avg_pdp(pipe, X_pdp, f, grid_resolution=50)  # <== Use X_pdp\n","        avg_smooth = pd.Series(avg).rolling(smooth_window, center=True, min_periods=1).mean().values\n","        ax.plot(grid_x, avg_smooth, linestyle=\"--\", linewidth=2, label=\"PDP (smoothed)\")\n","        ax.legend(loc=\"best\")\n","\n","    plt.tight_layout()\n","    png = out_dir / f\"pdp_ice_{model_name}_{f.replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_')}.png\"\n","    plt.savefig(png, dpi=220)\n","    plt.close()\n","    print(\"Saved ->\", png)"],"metadata":{"id":"XyaNzkY-07PQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_all = pd.concat([X_reg[feature_cols], X_test[feature_cols]], axis=0, ignore_index=True)\n","\n","#Consistent float conversion as 1D (prevent errors with object columns during grid creation)\n","X_all_pdp = X_all.copy()\n","cont_cols = set([c for pair in feat_2d for c in pair])\n","def _coerce_float_col(df, col):\n","    if col in df.columns:\n","        if df[col].dtype == 'O':\n","            s = df[col].astype(str).str.replace(r'[^0-9.\\-]+', '', regex=True)\n","            df[col] = pd.to_numeric(s, errors='coerce')\n","        if np.issubdtype(df[col].dtype, np.number):\n","            df[col] = df[col].fillna(df[col].median())\n","\n","for c in cont_cols:\n","    _coerce_float_col(X_all_pdp, c)\n","\n","#2D Interaction PDP (with robustness checks)\n","for f1, f2 in feat_2d:\n","    #Column existence check\n","    if (f1 not in X_all_pdp.columns) or (f2 not in X_all_pdp.columns):\n","        print(f\"[Skip] 2D: {(f1, f2)} — Column missing.\")\n","        continue\n","\n","    #Unique value check: Skip if either column has <2 unique values\n","    nunique1 = pd.Series(X_all_pdp[f1]).nunique(dropna=True)\n","    nunique2 = pd.Series(X_all_pdp[f2]).nunique(dropna=True)\n","    if (nunique1 < 2) or (nunique2 < 2):\n","        print(f\"[Skip] 2D: {(f1, f2)} — Insufficient unique values (nunique: {nunique1}, {nunique2}), cannot generate contour.\")\n","        continue\n","\n","    #Plotting\n","    fig, ax = plt.subplots(figsize=(5.8, 4.6))\n","    try:\n","        PartialDependenceDisplay.from_estimator(\n","            estimator=pipe,\n","            X=X_all_pdp,\n","            features=[(f1, f2)],\n","            kind=\"average\",\n","            grid_resolution=30,\n","            ax=ax\n","        )\n","        ax.set_title(f\"2D PDP (interaction) — {model_name} — {f1} × {f2}\")\n","        plt.tight_layout()\n","        png = out_dir / f\"pdp2d_{model_name}_{f1.replace(' ','_')}_x_{f2.replace(' ','_')}.png\"\n","        plt.savefig(png, dpi=220)\n","        plt.close()\n","        print(\"Saved ->\", png)\n","    except Exception as e:\n","        # Skip and alert if any extreme cases occur\n","        print(f\"[Skip] 2D: {(f1, f2)} — Plotting error: {e}\")"],"metadata":{"id":"zxmDhJH507b9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------    5.4 Residual Plot: Residuals vs Predicted (for RF+FS Optuna regression)     --------------------------------------------------------------------\n","\n","#Read the prediction results\n","pred_path = '/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_fs_optuna/rf_fs_optuna_test_predictions.csv'\n","df = pd.read_csv(pred_path)\n","\n","y_true = df['y_true'].to_numpy()\n","y_pred = df['y_pred'].to_numpy()\n","\n","resid   = y_pred - y_true\n","abs_err = np.abs(resid)\n","\n","#Calculate some indicators to facilitate the annotations in the chart.\n","mae   = abs_err.mean()\n","rmse  = np.sqrt(np.mean(resid**2))\n","r2    = 1.0 - np.sum(resid**2) / np.sum((y_true - y_true.mean())**2)\n","bias  = resid.mean()\n","\n","#Prepare for LOWESS smoothing\n","x = y_pred\n","y = resid\n","try:\n","    from statsmodels.nonparametric.smoothers_lowess import lowess\n","    low = lowess(y, x, frac=0.35, it=0, return_sorted=True)\n","    xs, ys = low[:, 0], low[:, 1]\n","except Exception:\n","    order = np.argsort(x)\n","    xs = x[order]\n","    ys = (pd.Series(y[order])\n","            .rolling(window=max(15, len(y)//20), center=True, min_periods=1)\n","            .median()\n","            .to_numpy())\n","\n","#Spearman correlation: Residuals vs. Predicted values\n","try:\n","    from scipy.stats import spearmanr\n","    rho, pval = spearmanr(np.abs(y), x)\n","    het_text = f\"|res|~pred ρ={rho:.2f}, p={pval:.3f}\"\n","except Exception:\n","    het_text = \"\"\n"],"metadata":{"id":"CyYQE8fIctd5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot\n","plt.figure(figsize=(7.2, 5.2))\n","plt.scatter(x, y, s=26, alpha=0.55, edgecolor='none')\n","plt.axhline(0, linestyle='--', linewidth=1)\n","for k in [1, 2, 3]:\n","    plt.axhline(+k, linestyle=':', linewidth=0.9)\n","    plt.axhline(-k, linestyle=':', linewidth=0.9)\n","\n","plt.plot(xs, ys, linewidth=2)\n","plt.xlabel('Predicted LOS (days)')\n","plt.ylabel('Residual (y_pred - y_true) [days]')\n","plt.title('Residual vs Predicted — RF + FS (Optuna)')\n","\n","#Bottom annotation of key indicators\n","txt = f\"MAE={mae:.2f} d | RMSE={rmse:.2f} d | R²={r2:.3f} | Bias={bias:+.2f} d\"\n","plt.gcf().text(0.5, -0.08, txt, ha='center', va='top')\n","if het_text:\n","    plt.gcf().text(0.99, -0.08, het_text, ha='right', va='top')\n","\n","plt.tight_layout()\n"],"metadata":{"id":"xkx-9ZtGctrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Load files\n","pred_path = '/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_fs_optuna/rf_fs_optuna_test_predictions.csv'\n","df = pd.read_csv(pred_path)\n","\n","y_true = df['y_true'].to_numpy()\n","y_pred = df['y_pred'].to_numpy()\n","resid  = y_pred - y_true\n","abs_err = np.abs(resid)\n","\n","#Calculate some indicators to facilitate the annotations in the chart.\n","mae  = abs_err.mean()\n","rmse = np.sqrt(np.mean(resid**2))\n","r2   = 1 - np.sum(resid**2)/np.sum((y_true - y_true.mean())**2)\n","bias = resid.mean()\n","\n","#Prepare for LOWESS smoothing\n","xs, ys = None, None\n","try:\n","    from statsmodels.nonparametric.smoothers_lowess import lowess\n","    low = lowess(resid, y_pred, frac=0.35, it=0, return_sorted=True)\n","    xs, ys = low[:, 0], low[:, 1]\n","except Exception:\n","    pass\n","\n","#Heteroscedasticity correlation (|res| ~ pred, Spearman)\n","het_text = \"\"\n","try:\n","    from scipy.stats import spearmanr\n","    rho, pval = spearmanr(np.abs(resid), y_pred)\n","    het_text = f\" | |res|~pred ρ={rho:.2f}, p={pval:.3f}\"\n","except Exception:\n","    pass\n","\n","out_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/interpretability/random_forest_fs_optuna/residuals')\n","out_dir.mkdir(parents=True, exist_ok=True)\n"],"metadata":{"id":"O11j7bbGct3U"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Draw the plot\n","fig, ax = plt.subplots(figsize=(7.6, 5.2), constrained_layout=False)\n","ax.scatter(y_pred, resid, s=26, alpha=0.55, linewidths=0)\n","\n","if xs is not None:\n","    ax.plot(xs, ys, linewidth=2)\n","\n","ax.axhline(0, ls='--', lw=1)\n","for k in (1,2,3):\n","    ax.axhline(+k, ls=':', lw=0.9)\n","    ax.axhline(-k, ls=':', lw=0.9)\n","\n","ax.set_xlabel('Predicted LOS (days)')\n","ax.set_ylabel('Residual (y_pred - y_true) [days]')\n","ax.set_title('Residual vs Predicted — RF + FS (Optuna)')\n","\n","#The annotation is placed in the lower left corner of the figure.\n","txt = f\"MAE={mae:.2f} d | RMSE={rmse:.2f} d | R²={r2:.3f} | Bias={bias:+.2f} d{het_text}\"\n","ax.text(0.01, 0.02, txt, transform=ax.transAxes, ha='left', va='bottom')\n","\n","#Properly add bottom/right margins to avoid element compression\n","fig.subplots_adjust(left=0.12, right=0.98, top=0.92, bottom=0.12)\n","fig.savefig(out_dir/'residual_vs_pred.png', dpi=300, bbox_inches='tight')\n","plt.close(fig)"],"metadata":{"id":"ICtYv7tccuDd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# sqrt(|residual|) vs predicted\n","fig, ax = plt.subplots(figsize=(7.6, 5.2))\n","ax.scatter(y_pred, np.sqrt(np.abs(resid)), s=26, alpha=0.55, linewidths=0)\n","ax.set_xlabel('Predicted LOS (days)')\n","ax.set_ylabel('sqrt(|residual|)')\n","ax.set_title('Scale–Location — RF + FS (Optuna)')\n","fig.subplots_adjust(left=0.12, right=0.98, top=0.92, bottom=0.12)\n","fig.savefig(out_dir/'scale_location.png', dpi=300, bbox_inches='tight')\n","plt.close(fig)"],"metadata":{"id":"r1BhiwFMJ2DW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Divide the values according to the predicted values into 10 equal quantile buckets,\n","#and calculate the MAE for each bucket.\n","tmp = pd.DataFrame({'y_pred': y_pred, 'abs_err': abs_err})\n","tmp['bin'] = pd.qcut(tmp['y_pred'], q=10, duplicates='drop')\n","bybin = tmp.groupby('bin', observed=True).agg(mean_mae=('abs_err','mean'), n=('abs_err','size')).reset_index()\n","\n","#Save\n","bybin.to_csv(out_dir/'mae_by_pred_decile.csv', index=False)\n","\n","#Bar chart\n","fig, ax = plt.subplots(figsize=(8.0, 4.2))\n","ax.bar(range(len(bybin)), bybin['mean_mae'])\n","ax.set_xticks(range(len(bybin)))\n","ax.set_xticklabels([f\"{i+1}\" for i in range(len(bybin))])\n","ax.set_xlabel('Predicted LOS decile (1=lowest)')\n","ax.set_ylabel('MAE (days)')\n","ax.set_title('MAE by Predicted LOS Decile — RF + FS (Optuna)')\n","fig.tight_layout()\n","fig.savefig(out_dir/'mae_by_pred_decile.png', dpi=300, bbox_inches='tight')\n","plt.close(fig)"],"metadata":{"id":"KasST-yzcuU2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------    5.4.2 Residual Plot: Residual Histogram + QQ Plot      --------------------------------------------------------------------\n","\n","pred_path = '/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/random_forest_fs_optuna/rf_fs_optuna_test_predictions.csv'\n","df = pd.read_csv(pred_path)\n","\n","y_true = df['y_true'].to_numpy()\n","y_pred = df['y_pred'].to_numpy()\n","resid  = y_pred - y_true\n","\n","bias = float(np.mean(resid))\n","std  = float(np.std(resid, ddof=1))\n","\n","#Skewness/Kurtosis & Normality Test\n","skew = kurt = None\n","test_txt = \"\"\n","try:\n","    from scipy.stats import skew as sk, kurtosis, shapiro, normaltest\n","    skew  = float(sk(resid, bias=False))\n","    kurt  = float(kurtosis(resid, fisher=True, bias=False))\n","    r4test = resid if len(resid) <= 5000 else np.random.default_rng(42).choice(resid, 5000, replace=False)\n","    W, p_shap = shapiro(r4test)\n","    K2, p_k2  = normaltest(resid)\n","    test_txt  = f\"Shapiro p={p_shap:.3g} | D’Agostino K² p={p_k2:.3g}\"\n","except Exception:\n","    pass\n","\n","out_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/interpretability/random_forest_fs_optuna/residuals')\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","#Residual histogram (with the appropriate number of bins determined by the Freedman-Diaconis rule)\n","def fd_bins(x):\n","    x = np.asarray(x)\n","    n = x.size\n","    if n < 2:\n","        return 'auto'\n","    q75, q25 = np.percentile(x, [75, 25])\n","    iqr = q75 - q25\n","    bw  = 2 * iqr / (n ** (1/3))\n","    if bw <= 0:\n","        return 'auto'\n","    bins = int(np.ceil((x.max() - x.min()) / bw))\n","    return max(bins, 10)\n","\n","bins = fd_bins(resid)\n","\n","fig, ax = plt.subplots(figsize=(7.2, 4.6))\n","ax.hist(resid, bins=bins)\n","ax.axvline(0, ls='--', lw=1)\n","ax.axvline(bias, ls=':', lw=1)\n","ax.set_xlabel('Residual (y_pred - y_true) [days]')\n","ax.set_ylabel('Frequency')\n","ax.set_title('Residual Histogram — RF + FS (Optuna)')\n","\n","meta = f\"mean={bias:+.2f} d | std={std:.2f} d\"\n","if (skew is not None) and (kurt is not None):\n","    meta += f\" | skew={skew:+.2f} | kurt={kurt:+.2f}\"\n","if test_txt:\n","    meta += f\"\\n{test_txt}\"\n","\n","#Place the instructions inside the axis to avoid overlapping with the coordinate axes.\n","ax.text(0.99, 0.98, meta, transform=ax.transAxes, ha='right', va='top')\n","fig.tight_layout()\n","fig.savefig(out_dir/'residual_histogram.png', dpi=300, bbox_inches='tight')\n","plt.close(fig)\n","\n","\n","\n","#QQ plot (Compared with Normal Distribution)\n","fig = plt.figure(figsize=(6.6, 6.6))\n","ax = fig.add_subplot(111)\n","probplot(resid, dist=\"norm\", plot=ax)\n","ax.set_title('Residual QQ Plot — RF + FS (Optuna)')\n","\n","note = \"Interpretation: deviation from line indicates non-normal tails/skew.\\nNormality is diagnostic only.\"\n","ax.text(0.5, -0.12, note, transform=ax.transAxes, ha='center', va='top')\n","if test_txt:\n","    ax.text(0.5, -0.18, test_txt, transform=ax.transAxes, ha='center', va='top')\n","\n","fig.tight_layout()\n","fig.savefig(out_dir/'residual_qq.png', dpi=300, bbox_inches='tight')\n","plt.close(fig)\n","\n","print(\"Saved:\")\n","print(\" -\", out_dir/'residual_histogram.png')\n","print(\" -\", out_dir/'residual_qq.png')\n"],"metadata":{"id":"x96ygSbfcugz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------    5.4.3 Residual Plot: Residual vs Age / BMI      --------------------------------------------------------------------\n","\n","#Output folder\n","out_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject/experiments/interpretability/random_forest_fs_optuna/residuals')\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","#small helpers\n","def find_col(df, candidates, fuzzy=True):\n","    \"\"\"Find a column in df by trying exact names in `candidates`;\n","    if not found and `fuzzy=True`, return the first column whose lowercased\n","    name contains any candidate substring.\"\"\"\n","    cmap = {c.lower(): c for c in df.columns}\n","    for c in candidates:\n","        key = c.lower()\n","        if key in cmap:\n","            return cmap[key]\n","    if fuzzy:\n","        lowers = [c.lower() for c in candidates]\n","        for c in df.columns:\n","            lc = c.lower()\n","            if any(k in lc for k in lowers):\n","                return c\n","    return None\n","\n","def lowess_xy(x, y, frac=0.35):\n","    \"\"\"Return x,y smoothed by LOWESS or a rolling-median fallback.\"\"\"\n","    try:\n","        from statsmodels.nonparametric.smoothers_lowess import lowess\n","        lo = lowess(y, x, frac=frac, it=0, return_sorted=True)\n","        return lo[:, 0], lo[:, 1]\n","    except Exception:\n","        order = np.argsort(x)\n","        xs = x[order]\n","        ys = (pd.Series(y[order])\n","                .rolling(window=max(15, len(y)//20), center=True, min_periods=1)\n","                .median()\n","                .to_numpy())\n","        return xs, ys\n","\n","def bootstrap_ci_mean(values, B=1000, seed=42):\n","    \"\"\"Bootstrap 95% CI for the mean.\"\"\"\n","    v = np.asarray(values, dtype=float)\n","    v = v[~np.isnan(v)]\n","    if v.size == 0:\n","        return np.nan, np.nan\n","    rng = np.random.default_rng(seed)\n","    stats = []\n","    n = v.size\n","    for _ in range(B):\n","        idx = rng.integers(0, n, n)\n","        stats.append(np.mean(v[idx]))\n","    lo, hi = np.percentile(stats, [2.5, 97.5])\n","    return float(lo), float(hi)\n","\n","def spearman_abs_err(x, e):\n","    \"\"\"Spearman correlation between |residual| and x.\"\"\"\n","    try:\n","        from scipy.stats import spearmanr\n","        rho, p = spearmanr(np.abs(e), x)\n","        return float(rho), float(p)\n","    except Exception:\n","        return np.nan, np.nan\n","\n","#main plotting routine\n","def plot_residual_vs_feature(x_raw, resid, var_name, is_bmi=False, fname_prefix='var'):\n","    \"\"\"\n","    Make (1) scatter + LOWESS of residuals vs feature\n","         (2) grouped boxplot with mean ± 95% CI\n","    \"\"\"\n","    #1) sanitize inputs\n","    x = pd.to_numeric(pd.Series(x_raw), errors='coerce').to_numpy()\n","    r = np.asarray(resid, dtype=float)\n","    m = ~np.isnan(x) & ~np.isnan(r)\n","    x, r = x[m], r[m]\n","\n","    #2) left panel: scatter + LOWESS\n","    xs, ys = lowess_xy(x, r, frac=0.35)\n","    rho, p  = spearman_abs_err(x, r)\n","\n","    #3) right panel: binning and per-bin CI\n","    if is_bmi:\n","        bins   = [0, 18.5, 25, 30, 35, np.inf]\n","        labels = ['<18.5', '18.5–24.9', '25–29.9', '30–34.9', '≥35']\n","        g = pd.cut(x, bins=bins, labels=labels, right=False, include_lowest=True)\n","    else:\n","        #Use quantile bins; fallback to equal-width if duplicates\n","        try:\n","            g = pd.qcut(x, q=5, duplicates='drop')\n","        except Exception:\n","            g = pd.cut(x, bins=5)\n","\n","    tmp = pd.DataFrame({'g': g, 'r': r}).dropna()\n","\n","    #Per-group mean and CI — build explicitly to avoid pandas .apply shape traps\n","    means = tmp.groupby('g', observed=False)['r'].mean()\n","    rows = []\n","    for name, vals in tmp.groupby('g', observed=False)['r']:\n","        lo_ci, hi_ci = bootstrap_ci_mean(vals.values)\n","        rows.append({'g': name, 'lo': lo_ci, 'hi': hi_ci})\n","    ci = pd.DataFrame(rows).set_index('g').loc[means.index]  # align order\n","\n","    #4) plot two panels\n","    fig, axes = plt.subplots(1, 2, figsize=(12, 4.5), gridspec_kw={'wspace': 0.25})\n","\n","    #(left) scatter + LOWESS\n","    ax = axes[0]\n","    ax.scatter(x, r, s=24, alpha=0.55, edgecolor='none')\n","    ax.plot(xs, ys, lw=2)\n","    ax.axhline(0, ls='--', lw=1)\n","    ax.set_xlabel(var_name)\n","    ax.set_ylabel('Residual (y_pred - y_true) [days]')\n","    ax.set_title(f'Residual vs {var_name} — LOWESS')\n","    if not np.isnan(rho):\n","        ax.text(0.99, 0.98, f\"|res|~{var_name} ρ={rho:.2f}, p={p:.3f}\",\n","                transform=ax.transAxes, ha='right', va='top')\n","\n","    #(right) grouped boxplot + mean ± 95% CI\n","    ax = axes[1]\n","    #enforce categorical order to match 'means'\n","    tmp['g'] = pd.Categorical(tmp['g'], categories=list(means.index), ordered=True)\n","    tmp.boxplot(column='r', by='g', ax=ax, grid=False)\n","    xpos = np.arange(1, len(means) + 1)\n","    ax.errorbar(\n","        xpos,\n","        means.values,\n","        yerr=[means.values - ci['lo'].values, ci['hi'].values - means.values],\n","        fmt='o', capsize=3, lw=1.5\n","    )\n","    ax.axhline(0, ls='--', lw=1)\n","    ax.set_xlabel(var_name + ' (binned)')\n","    ax.set_ylabel('Residual (days)')\n","    ax.set_title(f'{var_name} segments — mean ± 95% CI')\n","    fig.suptitle('')\n","    fig.tight_layout()\n","\n","    fpath = out_dir / f'{fname_prefix}_residual_lowess_box.png'\n","    fig.savefig(fpath, dpi=300, bbox_inches='tight')\n","    plt.close(fig)\n","    print(\"Saved →\", fpath)\n","\n","#locate Age / BMI columns and plot\n","age_candidates = ['age', 'age_years', 'patient_age']\n","bmi_candidates = ['bmi', 'body_mass_index', 'bmi_value']\n","\n","col_age = find_col(X_test, age_candidates, fuzzy=True)\n","col_bmi = find_col(X_test, bmi_candidates, fuzzy=True)\n","\n","if col_age is not None:\n","    plot_residual_vs_feature(X_test[col_age].values, resid, var_name='Age', is_bmi=False, fname_prefix='age')\n","else:\n","    print(\"Age column not found; candidates tried:\", age_candidates)\n","\n","if col_bmi is not None:\n","    plot_residual_vs_feature(X_test[col_bmi].values, resid, var_name='BMI', is_bmi=True, fname_prefix='bmi')\n","else:\n","    print(\"BMI column not found; candidates tried:\", bmi_candidates)"],"metadata":{"id":"sVT5PF21cu4s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------    5.5 Classification RF     --------------------------------------------------------------------\n","\n","#-----------------------------------------------------------------    5.5.1 Calibration plot + Brier score for RF (binary long-stay >= 3 days)     --------------------------------------------------------------------\n","\n","\n","base_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject')\n","data_dir = base_dir / 'experiments' / 'regression_baselines'\n","best_param_path = base_dir / 'experiments' / 'rf_longstay_binary' / 'best_params.json'\n","out_dir = base_dir / 'experiments' / 'interpretability' / 'random_forest_cls' / 'calibration'\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","#Load data\n","X_reg  = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg  = joblib.load(data_dir / 'y_reg.joblib')\n","X_test = joblib.load(data_dir / 'X_test.joblib')\n","y_test = joblib.load(data_dir / 'y_test.joblib')\n","\n","with open(data_dir / 'feature_lists.json', 'r') as f:\n","    feat = json.load(f)\n","numeric_features     = feat['numeric']\n","categorical_features = feat['categororical'] if 'categororical' in feat else feat['categorical']\n","feature_cols         = numeric_features + categorical_features\n","\n","#Make binary labels (>= 3 days = 1)\n","LONG_STAY_DAYS = 3\n","y_bin       = (np.asarray(y_reg).ravel()  >= LONG_STAY_DAYS).astype(int)\n","y_bin_test  = (np.asarray(y_test).ravel() >= LONG_STAY_DAYS).astype(int)\n","\n","#Preprocessing (same as regression part)\n","def clean_numeric_arr(arr):\n","    \"\"\"Strip non-numeric chars and cast to float; keeps array shape.\"\"\"\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(lambda x: pd.to_numeric(re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce')).values\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   FunctionTransformer(clean_numeric_arr, feature_names_out='one-to-one')),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","\n","#OneHotEncoder API changed across sklearn versions; handle both.\n","try:\n","    categorical_pipe = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","except TypeError:\n","    categorical_pipe = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse=False))\n","    ])\n","\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features)\n","])\n","\n","#RF classifier pipeline (prep -> VT -> KBest -> RF)\n","def build_rf_clf(percentile, n_estimators, max_depth, max_features,\n","                 min_samples_split, min_samples_leaf, bootstrap):\n","    return Pipeline([\n","        ('prep',  preprocessor),\n","        ('vt',    VarianceThreshold(0.0)),\n","        ('kbest', SelectPercentile(mutual_info_classif, percentile=percentile)),\n","        ('model', RandomForestClassifier(\n","            n_estimators=n_estimators, max_depth=max_depth, max_features=max_features,\n","            min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n","            bootstrap=bootstrap, class_weight='balanced', random_state=42, n_jobs=-1\n","        ))\n","    ])\n","\n","#Load best params if available; otherwise fall back to a solid default.\n","if best_param_path.exists():\n","    with open(best_param_path, 'r') as f:\n","        best_meta = json.load(f)\n","    best_params = best_meta.get('best_params', best_meta)\n","else:\n","    best_params = dict(\n","        percentile=70, n_estimators=600, max_depth=None,\n","        max_features='sqrt', min_samples_split=4, min_samples_leaf=1, bootstrap=True\n","    )\n","\n","rf_pipe = build_rf_clf(**best_params)\n","\n","#Train/valid split for calibration\n","X_tr, X_val, y_tr, y_val = train_test_split(\n","    X_reg[feature_cols], y_bin, test_size=0.2, random_state=42, stratify=y_bin\n",")\n","\n","#Fit RF on train\n","rf_pipe.fit(X_tr, y_tr)\n","\n","#Uncalibrated probabilities on test\n","proba_uncal = rf_pipe.predict_proba(X_test[feature_cols])[:, 1]\n","\n","#Fit isotonic calibrator on validation data\n","cal = CalibratedClassifierCV(rf_pipe, method='isotonic', cv='prefit')\n","cal.fit(X_val[feature_cols], y_val)\n","\n","#Calibrated probabilities on test\n","proba_cal = cal.predict_proba(X_test[feature_cols])[:, 1]\n","\n","#Brier score\n","brier_uncal = brier_score_loss(y_bin_test, proba_uncal, pos_label=1)\n","brier_cal   = brier_score_loss(y_bin_test, proba_cal,   pos_label=1)\n","print(f\"Brier score (uncalibrated RF): {brier_uncal:.4f}\")\n","print(f\"Brier score (RF + isotonic):   {brier_cal:.4f}\")\n","\n","#Calibration curves\n","#Use quantile bins so each bin has similar number of samples\n","prob_true_uncal, prob_pred_uncal = calibration_curve(y_bin_test, proba_uncal, n_bins=10, strategy='quantile')\n","prob_true_cal,   prob_pred_cal   = calibration_curve(y_bin_test, proba_cal,   n_bins=10, strategy='quantile')\n","\n","fig, ax = plt.subplots(figsize=(6.4, 5.6))\n","\n","#Perfectly calibrated reference\n","ax.plot([0, 1], [0, 1], linestyle='--', linewidth=1, label='Perfect calibration')\n","\n","#Model curves\n","ax.plot(prob_pred_uncal, prob_true_uncal, marker='o', linewidth=2,\n","        label=f'RF (uncal.) — Brier={brier_uncal:.3f}')\n","ax.plot(prob_pred_cal,   prob_true_cal,   marker='o', linewidth=2,\n","        label=f'RF + Isotonic — Brier={brier_cal:.3f}')\n","\n","ax.set_xlabel('Predicted probability (positive)')\n","ax.set_ylabel('Observed fraction of positives')\n","ax.set_title('Calibration plot — Random Forest (binary long-stay ≥ 3d)')\n","ax.legend(loc='upper left')\n","ax.grid(True, alpha=0.3)\n","fig.tight_layout()\n","\n","fpath = out_dir / 'calibration_rf_vs_isotonic.png'\n","fig.savefig(fpath, dpi=300, bbox_inches='tight')\n","plt.close(fig)\n","print(\"Saved figure →\", fpath)\n","\n","#Probability histogram under the same out_dir\n","fig, ax = plt.subplots(figsize=(6.4, 3.0))\n","ax.hist(proba_uncal, bins=20, alpha=0.6, label='RF (uncal.)')\n","ax.hist(proba_cal,   bins=20, alpha=0.6, label='RF + Isotonic')\n","ax.set_xlabel('Predicted probability (positive)')\n","ax.set_ylabel('Count')\n","ax.set_title('Predicted probability distribution (test)')\n","ax.legend()\n","fig.tight_layout()\n","fpath2 = out_dir / 'proba_hist_rf_vs_isotonic.png'\n","fig.savefig(fpath2, dpi=300, bbox_inches='tight')\n","plt.close(fig)\n","print(\"Saved figure →\", fpath2)"],"metadata":{"id":"sMv2hCPTZqja"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------    5.5.2 ROC curve + AUC, PR curve + AP for RF (binary long-stay >= 3 days)     --------------------------------------------------------------------\n","\n","\n","base_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject')\n","data_dir = base_dir / 'experiments' / 'regression_baselines'\n","best_param_path = base_dir / 'experiments' / 'rf_longstay_binary' / 'best_params.json'\n","out_dir = base_dir / 'experiments' / 'interpretability' / 'random_forest_cls' / 'roc_pr'\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","X_reg  = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg  = joblib.load(data_dir / 'y_reg.joblib')\n","X_test = joblib.load(data_dir / 'X_test.joblib')\n","y_test = joblib.load(data_dir / 'y_test.joblib')\n","\n","with open(data_dir / 'feature_lists.json', 'r') as f:\n","    feat = json.load(f)\n","numeric_features     = feat['numeric']\n","categorical_features = feat['categororical'] if 'categororical' in feat else feat['categorical']\n","feature_cols         = numeric_features + categorical_features\n","\n","#Make binary labels (>= 3 days = 1)\n","LONG_STAY_DAYS = 3\n","y_bin       = (np.asarray(y_reg).ravel()  >= LONG_STAY_DAYS).astype(int)\n","y_bin_test  = (np.asarray(y_test).ravel() >= LONG_STAY_DAYS).astype(int)\n","pos_rate    = float(y_bin_test.mean())\n","\n","# Preprocessing identical to regression part\n","def clean_numeric_arr(arr):\n","    \"\"\"Strip non-numeric chars and cast to float; keeps array shape.\"\"\"\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(lambda x: pd.to_numeric(re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce')).values\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   FunctionTransformer(clean_numeric_arr, feature_names_out='one-to-one')),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","\n","#OneHotEncoder API fallback for different sklearn versions\n","try:\n","    categorical_pipe = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","except TypeError:\n","    categorical_pipe = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse=False))\n","    ])\n","\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features)\n","])\n","\n","#RF classifier pipeline (prep -> VT -> KBest -> RF)\n","def build_rf_clf(percentile, n_estimators, max_depth, max_features,\n","                 min_samples_split, min_samples_leaf, bootstrap):\n","    return Pipeline([\n","        ('prep',  preprocessor),\n","        ('vt',    VarianceThreshold(0.0)),\n","        ('kbest', SelectPercentile(mutual_info_classif, percentile=percentile)),\n","        ('model', RandomForestClassifier(\n","            n_estimators=n_estimators, max_depth=max_depth, max_features=max_features,\n","            min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n","            bootstrap=bootstrap, class_weight='balanced', random_state=42, n_jobs=-1\n","        ))\n","    ])\n","\n","#Load tuned params if available; otherwise use a robust default\n","if best_param_path.exists():\n","    with open(best_param_path, 'r') as f:\n","        best_meta = json.load(f)\n","    best_params = best_meta.get('best_params', best_meta)\n","else:\n","    best_params = dict(\n","        percentile=70, n_estimators=600, max_depth=None,\n","        max_features='sqrt', min_samples_split=4, min_samples_leaf=1, bootstrap=True\n","    )\n","\n","rf_pipe = build_rf_clf(**best_params)\n","\n","#Train/valid split; fit base model; set up isotonic calibration ----------\n","X_tr, X_val, y_tr, y_val = train_test_split(\n","    X_reg[feature_cols], y_bin, test_size=0.2, random_state=42, stratify=y_bin\n",")\n","\n","#Fit RF on train\n","rf_pipe.fit(X_tr, y_tr)\n","\n","#Uncalibrated probabilities on test\n","proba_uncal = rf_pipe.predict_proba(X_test[feature_cols])[:, 1]\n","\n","#Isotonic calibration (cv='prefit' means base model already trained)\n","cal = CalibratedClassifierCV(rf_pipe, method='isotonic', cv='prefit')\n","cal.fit(X_val[feature_cols], y_val)\n","\n","#Calibrated probabilities on test\n","proba_cal = cal.predict_proba(X_test[feature_cols])[:, 1]\n","\n","# ---------- ROC + AUC ----------\n","fpr_u, tpr_u, _ = roc_curve(y_bin_test, proba_uncal)\n","fpr_c, tpr_c, _ = roc_curve(y_bin_test, proba_cal)\n","\n","auc_u = roc_auc_score(y_bin_test, proba_uncal)\n","auc_c = roc_auc_score(y_bin_test, proba_cal)\n","\n","fig, ax = plt.subplots(figsize=(6.2, 5.6))\n","ax.plot([0, 1], [0, 1], linestyle='--', linewidth=1, label='Chance')\n","ax.plot(fpr_u, tpr_u, linewidth=2, label=f'RF (uncal.) — AUC={auc_u:.3f}')\n","ax.plot(fpr_c, tpr_c, linewidth=2, label=f'RF + Isotonic — AUC={auc_c:.3f}')\n","ax.set_xlabel('False Positive Rate')\n","ax.set_ylabel('True Positive Rate')\n","ax.set_title('ROC — Random Forest (binary long-stay ≥ 3d)')\n","ax.legend(loc='lower right')\n","ax.grid(True, alpha=0.3)\n","fig.tight_layout()\n","roc_path = out_dir / 'roc_rf_uncal_vs_cal.png'\n","fig.savefig(roc_path, dpi=300, bbox_inches='tight')\n","plt.close(fig)\n","print(\"Saved ROC →\", roc_path)\n","\n","# ----------  PR + AP ----------\n","prec_u, rec_u, _ = precision_recall_curve(y_bin_test, proba_uncal)\n","prec_c, rec_c, _ = precision_recall_curve(y_bin_test, proba_cal)\n","\n","ap_u = average_precision_score(y_bin_test, proba_uncal)\n","ap_c = average_precision_score(y_bin_test, proba_cal)\n","\n","fig, ax = plt.subplots(figsize=(6.2, 5.6))\n","#Baseline is the positive rate (no-skill classifier)\n","ax.hlines(pos_rate, xmin=0, xmax=1, linestyles='--', linewidth=1, label=f'Baseline (pos rate={pos_rate:.2f})')\n","ax.plot(rec_u, prec_u, linewidth=2, label=f'RF (uncal.) — AP={ap_u:.3f}')\n","ax.plot(rec_c, prec_c, linewidth=2, label=f'RF + Isotonic — AP={ap_c:.3f}')\n","ax.set_xlabel('Recall')\n","ax.set_ylabel('Precision')\n","ax.set_title('Precision–Recall — Random Forest (binary long-stay ≥ 3d)')\n","ax.set_xlim(0, 1)\n","ax.set_ylim(0, 1.02)\n","ax.legend(loc='upper right')\n","ax.grid(True, alpha=0.3)\n","fig.tight_layout()\n","pr_path = out_dir / 'pr_rf_uncal_vs_cal.png'\n","fig.savefig(pr_path, dpi=300, bbox_inches='tight')\n","plt.close(fig)\n","print(\"Saved PR →\", pr_path)\n","\n","#Print numeric metrics\n","print(f\"\\n[Metrics on TEST]\")\n","print(f\"ROC-AUC  (uncalibrated): {auc_u:.4f}\")\n","print(f\"ROC-AUC  (calibrated)  : {auc_c:.4f}\")\n","print(f\"PR-AP    (uncalibrated): {ap_u:.4f}\")\n","print(f\"PR-AP    (calibrated)  : {ap_c:.4f}\")\n","print(f\"Positive rate (baseline for PR): {pos_rate:.4f}\")\n","\n"],"metadata":{"id":"ZllS0x7hZqx2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------    5.5.3 Threshold sensitivity: threshold vs Precision/Recall/F1   --------------------------------------------------------------------\n","\n","\n","base_dir = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject')\n","data_dir = base_dir / 'experiments' / 'regression_baselines'\n","best_param_path = base_dir / 'experiments' / 'rf_longstay_binary' / 'best_params.json'  # if created before\n","out_dir = base_dir / 'experiments' / 'interpretability' / 'random_forest_cls' / 'threshold_sensitivity'\n","out_dir.mkdir(parents=True, exist_ok=True)\n","\n","X_reg  = joblib.load(data_dir / 'X_reg.joblib')\n","y_reg  = joblib.load(data_dir / 'y_reg.joblib')\n","X_test = joblib.load(data_dir / 'X_test.joblib')\n","y_test = joblib.load(data_dir / 'y_test.joblib')\n","\n","with open(data_dir / 'feature_lists.json', 'r') as f:\n","    feat = json.load(f)\n","numeric_features     = feat['numeric']\n","categorical_features = feat['categororical'] if 'categororical' in feat else feat['categorical']\n","feature_cols         = numeric_features + categorical_features\n","\n","#Make binary labels (>= 3 days = 1)\n","LONG_STAY_DAYS = 3\n","y_bin      = (np.asarray(y_reg).ravel()  >= LONG_STAY_DAYS).astype(int)\n","y_bin_test = (np.asarray(y_test).ravel() >= LONG_STAY_DAYS).astype(int)\n","\n","#Preprocessing identical to regression part\n","def clean_numeric_arr(arr):\n","    \"\"\"Strip non-numeric chars and cast to float; keeps array shape.\"\"\"\n","    df0 = pd.DataFrame(arr)\n","    return df0.applymap(lambda x: pd.to_numeric(re.sub(r'[^0-9.\\-]+', '', str(x)), errors='coerce')).values\n","\n","numeric_pipe = Pipeline([\n","    ('clean',   FunctionTransformer(clean_numeric_arr, feature_names_out='one-to-one')),\n","    ('imputer', SimpleImputer(strategy='median')),\n","    ('scaler',  StandardScaler())\n","])\n","\n","# OneHotEncoder API fallback for different sklearn versions\n","try:\n","    categorical_pipe = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n","    ])\n","except TypeError:\n","    categorical_pipe = Pipeline([\n","        ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n","        ('ohe',     OneHotEncoder(handle_unknown='ignore', sparse=False))\n","    ])\n","\n","preprocessor = ColumnTransformer([\n","    ('num', numeric_pipe,     numeric_features),\n","    ('cat', categorical_pipe, categorical_features)\n","])\n","\n","#RF classifier pipeline (prep -> VT -> KBest -> RF)\n","def build_rf_clf(percentile, n_estimators, max_depth, max_features,\n","                 min_samples_split, min_samples_leaf, bootstrap):\n","    return Pipeline([\n","        ('prep',  preprocessor),\n","        ('vt',    VarianceThreshold(0.0)),\n","        ('kbest', SelectPercentile(mutual_info_classif, percentile=percentile)),\n","        ('model', RandomForestClassifier(\n","            n_estimators=n_estimators, max_depth=max_depth, max_features=max_features,\n","            min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,\n","            bootstrap=bootstrap, class_weight='balanced', random_state=42, n_jobs=-1\n","        ))\n","    ])\n","\n","#Load tuned params if available; otherwise use a robust default\n","if best_param_path.exists():\n","    with open(best_param_path, 'r') as f:\n","        best_meta = json.load(f)\n","    best_params = best_meta.get('best_params', best_meta)\n","else:\n","    best_params = dict(\n","        percentile=70, n_estimators=600, max_depth=None,\n","        max_features='sqrt', min_samples_split=4, min_samples_leaf=1, bootstrap=True\n","    )\n","\n","rf_pipe = build_rf_clf(**best_params)\n","\n","#Train/valid split; fit base model; isotonic calibration\n","X_tr, X_val, y_tr, y_val = train_test_split(\n","    X_reg[feature_cols], y_bin, test_size=0.2, random_state=42, stratify=y_bin\n",")\n","\n","#Fit RF on train\n","rf_pipe.fit(X_tr, y_tr)\n","\n","#Get probabilities\n","proba_val  = rf_pipe.predict_proba(X_val[feature_cols])[:, 1]\n","proba_test_uncal = rf_pipe.predict_proba(X_test[feature_cols])[:, 1]\n","\n","#Calibrate (optional but recommended to synchronize with previous sections)\n","cal = CalibratedClassifierCV(rf_pipe, method='isotonic', cv='prefit')\n","cal.fit(X_val[feature_cols], y_val)\n","proba_val_cal  = cal.predict_proba(X_val[feature_cols])[:, 1]\n","proba_test_cal = cal.predict_proba(X_test[feature_cols])[:, 1]\n","\n","#Choose which probabilities to analyze for thresholding\n","use_calibrated = True   # set to False if you want uncalibrated\n","proba_val_used  = proba_val_cal if use_calibrated else proba_val\n","proba_test_used = proba_test_cal if use_calibrated else proba_test_uncal\n","\n","#Sweep thresholds and compute metrics\n","def sweep_thresholds(y_true, proba, thresholds):\n","    \"\"\"Compute precision, recall, f1_pos, f1_weighted for each threshold.\"\"\"\n","    rows = []\n","    for thr in thresholds:\n","        y_hat = (proba >= thr).astype(int)\n","        prec = precision_score(y_true, y_hat, zero_division=0)\n","        rec  = recall_score(y_true, y_hat, zero_division=0)\n","        f1p  = f1_score(y_true, y_hat, average='binary', zero_division=0)\n","        f1w  = f1_score(y_true, y_hat, average='weighted', zero_division=0)\n","        acc  = accuracy_score(y_true, y_hat)\n","        rows.append((thr, prec, rec, f1p, f1w, acc))\n","    dfm = pd.DataFrame(rows, columns=['threshold','precision','recall','f1_pos','f1_weighted','accuracy'])\n","    return dfm\n","\n","thr_grid = np.linspace(0.00, 1.00, 201)\n","val_curve  = sweep_thresholds(y_val,  proba_val_used,  thr_grid)\n","test_curve = sweep_thresholds(y_bin_test, proba_test_used, thr_grid)\n","\n","#Pick best threshold on validation (maximize weighted-F1)\n","crit = 'f1_weighted'\n","best_idx = val_curve[crit].values.argmax()\n","best_thr = float(val_curve.iloc[best_idx]['threshold'])\n","best_val_metric = float(val_curve.iloc[best_idx][crit])\n","\n","# Apply best threshold on test\n","y_test_hat_best = (proba_test_used >= best_thr).astype(int)\n","prec_best = precision_score(y_bin_test, y_test_hat_best, zero_division=0)\n","rec_best  = recall_score(y_bin_test, y_test_hat_best, zero_division=0)\n","f1_best   = f1_score(y_bin_test, y_test_hat_best, average='binary', zero_division=0)\n","f1w_best  = f1_score(y_bin_test, y_test_hat_best, average='weighted', zero_division=0)\n","acc_best  = accuracy_score(y_bin_test, y_test_hat_best)\n","\n","print(f\"[Validation] best threshold by {crit} = {best_thr:.2f} | {crit}={best_val_metric:.3f}\")\n","print(f\"[Test @ best thr] Precision={prec_best:.3f} | Recall={rec_best:.3f} | F1(pos)={f1_best:.3f} \"\n","      f\"| F1(weighted)={f1w_best:.3f} | Acc={acc_best:.3f}\")\n","\n","#Plot: threshold vs Precision/Recall/F1(pos)\n","fig, ax = plt.subplots(figsize=(7.4, 5.2))\n","\n","ax.plot(val_curve['threshold'],  val_curve['precision'],  label='Precision (valid)',  linewidth=2)\n","ax.plot(val_curve['threshold'],  val_curve['recall'],     label='Recall (valid)',     linewidth=2)\n","ax.plot(val_curve['threshold'],  val_curve['f1_pos'],     label='F1 (pos, valid)',    linewidth=2)\n","\n","#show test curves in lighter style for reference\n","ax.plot(test_curve['threshold'], test_curve['precision'],  linestyle='--', alpha=0.5, label='Precision (test)')\n","ax.plot(test_curve['threshold'], test_curve['recall'],     linestyle='--', alpha=0.5, label='Recall (test)')\n","ax.plot(test_curve['threshold'], test_curve['f1_pos'],     linestyle='--', alpha=0.5, label='F1 (pos, test)')\n","\n","#vertical line = best threshold\n","ax.axvline(best_thr, color='k', linestyle=':', linewidth=1.5,\n","           label=f'Best thr (valid {crit}): {best_thr:.2f}')\n","\n","ax.set_xlabel('Decision threshold')\n","ax.set_ylabel('Score')\n","ax.set_title('Threshold sensitivity — Precision / Recall / F1 (positive class)\\nRF (binary long-stay ≥ 3d)')\n","ax.set_xlim(0, 1); ax.set_ylim(0, 1.02)\n","ax.grid(True, alpha=0.3)\n","ax.legend(loc='best', ncols=2)\n","fig.tight_layout()\n","\n","fig_path = out_dir / 'threshold_sensitivity_rf.png'\n","fig.savefig(fig_path, dpi=300, bbox_inches='tight')\n","plt.close(fig)\n","print(\"Saved plot →\", fig_path)\n","\n","#Save detailed tables\n","val_curve.to_csv(out_dir / 'threshold_sweep_valid.csv', index=False)\n","test_curve.to_csv(out_dir / 'threshold_sweep_test.csv',  index=False)\n","\n","with open(out_dir / 'chosen_threshold.json', 'w') as f:\n","    json.dump({\n","        'use_calibrated': use_calibrated,\n","        'criterion': crit,\n","        'best_thr_valid': best_thr,\n","        'valid_best_metric': best_val_metric,\n","        'test_at_best_thr': {\n","            'precision': float(prec_best),\n","            'recall':    float(rec_best),\n","            'f1_pos':    float(f1_best),\n","            'f1_weighted': float(f1w_best),\n","            'accuracy':  float(acc_best)\n","        }\n","    }, f, indent=2)\n","\n","print(\"Saved curves and chosen-threshold metadata to:\", out_dir)"],"metadata":{"id":"Z6XquHonZq-4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#-----------------------------------------------------------------    5.5.4 onfusion Matrix Heatmaps (RF binary long-stay >= 3 days)   --------------------------------------------------------------------\n","\n","\n","try:\n","    import seaborn as sns\n","    _HAS_SNS = True\n","except Exception:\n","    _HAS_SNS = False\n","\n","from sklearn.metrics import (\n","    confusion_matrix, classification_report,\n","    accuracy_score, precision_score, recall_score, f1_score\n",")\n","\n","BASE = Path('/content/drive/MyDrive/Colab Notebooks/LOSproject')\n","\n","PRED_CSV = BASE / 'experiments' / 'rf_longstay_binary' / 'rf_longstay_bin_test_predictions.csv'\n","\n","\n","THR_JSON_CANDIDATES = [\n","    BASE / 'experiments' / 'interpretability' / 'random_forest_cls' / 'threshold_sensitivity' / 'chosen_threshold.json',\n","    BASE / 'experiments' / 'rf_longstay_binary' / 'best_params.json',  # only if you also wrote threshold here\n","]\n","\n","#Output folder for CM plots\n","OUT_DIR = BASE / 'experiments' / 'interpretability' / 'random_forest_cls' / 'confusion_matrix'\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","#Load predictions (ground-truth + either saved y_pred or proba_pos) ----------\n","df = pd.read_csv(PRED_CSV)\n","if 'y_true' not in df.columns:\n","    raise ValueError(f\"'y_true' not found in {PRED_CSV}. Please check the file.\")\n","\n","y_true = df['y_true'].astype(int).to_numpy()\n","\n","#Decide how to get y_hat:\n","#1) Prefer the saved y_pred\n","if 'y_pred' in df.columns:\n","    y_hat = df['y_pred'].astype(int).to_numpy()\n","    thr_used = None  # unknown / not needed\n","    source_msg = \"Using saved y_pred from the predictions file.\"\n","#2) Otherwise, try to load a threshold and binarize proba_pos\n","elif 'proba_pos' in df.columns:\n","    proba = pd.to_numeric(df['proba_pos'], errors='coerce').to_numpy()\n","    thr_loaded = None\n","    for jp in THR_JSON_CANDIDATES:\n","        if jp.exists():\n","            with open(jp, 'r') as f:\n","                meta = json.load(f)\n","            thr_loaded = meta.get('best_thr', meta.get('best_thr_valid', None))\n","            if thr_loaded is not None:\n","                thr_loaded = float(thr_loaded)\n","                break\n","    if thr_loaded is None:\n","        thr_loaded = 0.50\n","\n","    y_hat = (proba >= thr_loaded).astype(int)\n","    thr_used = thr_loaded\n","    source_msg = f\"y_pred not found; used proba_pos with threshold={thr_loaded:.2f}.\"\n","else:\n","    raise ValueError(\n","        f\"Neither 'y_pred' nor 'proba_pos' found in {PRED_CSV}. \"\n","        \"Please re-save predictions with at least one of them.\"\n","    )\n","\n","print(source_msg)\n","\n","#Confusion matrices\n","labels = [0, 1]\n","class_names = ['Non-long-stay (<3d)', 'Long-stay (≥3d)']\n","\n","cm = confusion_matrix(y_true, y_hat, labels=labels)\n","cm_norm = cm.astype(float) / cm.sum(axis=1, keepdims=True)\n","\n","#Plot helper\n","def plot_cm(mat, title, fname, fmt='.0f', cmap='Blues'):\n","    \"\"\"Draw and save a confusion matrix heatmap.\"\"\"\n","    fig, ax = plt.subplots(figsize=(5.6, 4.8))\n","    if _HAS_SNS:\n","        sns.heatmap(\n","            mat, annot=True, fmt=fmt, cmap=cmap, cbar=True, ax=ax,\n","            xticklabels=class_names, yticklabels=class_names,\n","            annot_kws={'fontsize': 11}\n","        )\n","    else:\n","        im = ax.imshow(mat, cmap=cmap)\n","        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n","        #Add text annotations manually\n","        for i in range(mat.shape[0]):\n","            for j in range(mat.shape[1]):\n","                ax.text(j, i, format(mat[i, j], fmt if isinstance(mat[i, j], (int, float)) else 's'),\n","                        ha='center', va='center', color='black')\n","        ax.set_xticks(np.arange(len(class_names)))\n","        ax.set_yticks(np.arange(len(class_names)))\n","        ax.set_xticklabels(class_names)\n","        ax.set_yticklabels(class_names)\n","\n","    ax.set_xlabel('Predicted label')\n","    ax.set_ylabel('True label')\n","    ax.set_title(title)\n","    fig.tight_layout()\n","    path = OUT_DIR / fname\n","    fig.savefig(path, dpi=300, bbox_inches='tight')\n","    plt.close(fig)\n","    print(\"Saved plot →\", path)\n","\n","#Draw & save -\n","thr_info = \"\" if ( 'y_pred' in df.columns ) else f\" (thr={thr_used:.2f})\"\n","plot_cm(cm,      title=f'Confusion Matrix (counts) — RF binary long-stay{thr_info}',\n","        fname='cm_counts.png',     fmt='.0f')\n","plot_cm(cm_norm, title=f'Confusion Matrix (normalized by row) — RF binary long-stay{thr_info}',\n","        fname='cm_normalized.png', fmt='.2f')\n","\n","#Quick metrics for sanity check\n","acc  = accuracy_score(y_true, y_hat)\n","prec = precision_score(y_true, y_hat, zero_division=0)\n","rec  = recall_score(y_true, y_hat, zero_division=0)\n","f1p  = f1_score(y_true, y_hat, average='binary',   zero_division=0)\n","f1w  = f1_score(y_true, y_hat, average='weighted', zero_division=0)\n","\n","thr_print = \"saved y_pred\" if ('y_pred' in df.columns) else f\"thr={thr_used:.2f}\"\n","print(f\"[Test @ {thr_print}]  Acc={acc:.3f} | Precision={prec:.3f} | \"\n","      f\"Recall={rec:.3f} | F1(pos)={f1p:.3f} | F1(weighted)={f1w:.3f}\")\n","\n","print(\"\\nClassification report:\\n\",\n","      classification_report(y_true, y_hat, target_names=class_names, digits=3))\n"],"metadata":{"id":"UsnGg-ArZrwq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CMjtvs4BZsJL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ZpiNsfVcZsUh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"5Lccl0jDZsfj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Wzc5ujxGZstB"},"execution_count":null,"outputs":[]}]}
